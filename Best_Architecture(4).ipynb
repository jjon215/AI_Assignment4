{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Best Architecture(4).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjon215/MLHW_2/blob/master/Best_Architecture(4).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "6MfO_BJ2EDJB",
        "colab_type": "code",
        "outputId": "33d4856f-cac4-47b3-c8fc-ded1ab2bc341",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4706
        }
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import np_utils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "batch_size = 32\n",
        "num_classes = 10\n",
        "epochs = 100\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "x_train = list(x_train)\n",
        "y_train = list(y_train)\n",
        "\n",
        "val_loss = list()\n",
        "val_acc = list()\n",
        "\n",
        "for i in range(5):\n",
        "  x_val = np.array(x_train[i*10000:(i+1)*10000])\n",
        "  y_val = np.array(y_train[i*10000:(i+1)*10000])\n",
        "  \n",
        "  x_tra = np.array(x_train[0:i*10000]+x_train[(i+1)*10000:50000])\n",
        "  y_tra = np.array(y_train[0:i*10000]+y_train[(i+1)*10000:50000])\n",
        "  \n",
        "  \n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                 input_shape=x_tra.shape[1:]))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Conv2D(32, (3, 3)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Conv2D(64, (3, 3)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Conv2D(128, (2, 2), padding='same'))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Conv2D(128, (2, 2)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(1024))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(num_classes))\n",
        "  model.add(Activation('softmax'))\n",
        "\n",
        "  model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=Adam(lr=0.0001, decay=1e-6),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "  datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  \n",
        "        samplewise_center=False,  \n",
        "        featurewise_std_normalization=False, \n",
        "        samplewise_std_normalization=False,  \n",
        "        zca_whitening=False,  \n",
        "        zca_epsilon=1e-06, \n",
        "        rotation_range=0,  \n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,\n",
        "        shear_range=0., \n",
        "        zoom_range=0., \n",
        "        channel_shift_range=0.,  \n",
        "        fill_mode='nearest',\n",
        "        cval=0.,  \n",
        "        horizontal_flip=True,  \n",
        "        vertical_flip=False,  \n",
        "        rescale=None,\n",
        "        preprocessing_function=None,\n",
        "        data_format=None,\n",
        "        validation_split=0.0)\n",
        "\n",
        "  datagen.fit(x_tra)\n",
        "\n",
        "  for e in range(10):\n",
        "      batches = 0\n",
        "      for x_batch, y_batch in datagen.flow(x_tra, y_tra, batch_size=50000):\n",
        "          model.fit(x_batch, y_batch)\n",
        "          batches += 1\n",
        "          if batches >= 1:\n",
        "              break\n",
        "\n",
        "  \n",
        "  History = model.fit(x_tra, y_tra,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_val, y_val))\n",
        "  score = model.evaluate(x_val, y_val, verbose=0)\n",
        "  print('Test loss is:', score[0])\n",
        "  print('Test accuracy is:', score[1])\n",
        "  print(score)\n",
        "  print(History.history)\n",
        "\n",
        "  plotaccuracy = plt.plot(range(1,epochs+1),History.history['acc'],range(1,epochs+1),History.history['val_acc'])\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.legend(('Train Accuracy','Test Accuracy'))\n",
        "  plt.show(plotaccuracy)\n",
        "  \n",
        "  val_loss.append(History.history['val_loss'][-1])\n",
        "  val_acc.append(History.history['val_acc'][-1])\n",
        "\n",
        "avg_val_loss = 0\n",
        "avg_val_acc = 0\n",
        "for i in range(5):\n",
        "  avg_val_loss += val_loss[i]/5\n",
        "  avg_val_acc += val_acc[i]/5\n",
        "  \n",
        "print('Avgerage validation loss:',avg_val_loss)\n",
        "print('Average validation accuracy:',avg_val_acc)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/1\n",
            "40000/40000 [==============================] - 36s 889us/step - loss: 1.9862 - acc: 0.2494\n",
            "Epoch 1/1\n",
            "40000/40000 [==============================] - 34s 859us/step - loss: 1.7019 - acc: 0.3654\n",
            "Epoch 1/1\n",
            "40000/40000 [==============================] - 35s 874us/step - loss: 1.5803 - acc: 0.4128\n",
            "Epoch 1/1\n",
            "40000/40000 [==============================] - 33s 831us/step - loss: 1.4922 - acc: 0.4496\n",
            "Epoch 1/1\n",
            "40000/40000 [==============================] - 35s 865us/step - loss: 1.4232 - acc: 0.4791\n",
            "Epoch 1/1\n",
            "40000/40000 [==============================] - 36s 890us/step - loss: 1.3653 - acc: 0.5027\n",
            "Epoch 1/1\n",
            "40000/40000 [==============================] - 35s 867us/step - loss: 1.3189 - acc: 0.5238\n",
            "Epoch 1/1\n",
            "40000/40000 [==============================] - 33s 824us/step - loss: 1.2708 - acc: 0.5443\n",
            "Epoch 1/1\n",
            "40000/40000 [==============================] - 35s 865us/step - loss: 1.2338 - acc: 0.5551\n",
            "Epoch 1/1\n",
            "40000/40000 [==============================] - 33s 823us/step - loss: 1.1952 - acc: 0.5715\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "40000/40000 [==============================] - 36s 901us/step - loss: 1.0856 - acc: 0.6110 - val_loss: 0.9913 - val_acc: 0.6440\n",
            "Epoch 2/100\n",
            "40000/40000 [==============================] - 37s 918us/step - loss: 1.0424 - acc: 0.6309 - val_loss: 0.9389 - val_acc: 0.6673\n",
            "Epoch 3/100\n",
            "40000/40000 [==============================] - 38s 938us/step - loss: 1.0146 - acc: 0.6396 - val_loss: 0.9585 - val_acc: 0.6539\n",
            "Epoch 4/100\n",
            "40000/40000 [==============================] - 37s 930us/step - loss: 0.9819 - acc: 0.6501 - val_loss: 0.9181 - val_acc: 0.6703\n",
            "Epoch 5/100\n",
            "40000/40000 [==============================] - 37s 923us/step - loss: 0.9569 - acc: 0.6599 - val_loss: 0.8783 - val_acc: 0.6829\n",
            "Epoch 6/100\n",
            "40000/40000 [==============================] - 37s 932us/step - loss: 0.9330 - acc: 0.6690 - val_loss: 0.8514 - val_acc: 0.6944\n",
            "Epoch 7/100\n",
            "40000/40000 [==============================] - 37s 930us/step - loss: 0.9055 - acc: 0.6800 - val_loss: 0.8280 - val_acc: 0.7026\n",
            "Epoch 8/100\n",
            "40000/40000 [==============================] - 37s 933us/step - loss: 0.8886 - acc: 0.6863 - val_loss: 0.7998 - val_acc: 0.7181\n",
            "Epoch 9/100\n",
            "40000/40000 [==============================] - 37s 935us/step - loss: 0.8651 - acc: 0.6942 - val_loss: 0.7965 - val_acc: 0.7146\n",
            "Epoch 10/100\n",
            "40000/40000 [==============================] - 37s 935us/step - loss: 0.8469 - acc: 0.7011 - val_loss: 0.7883 - val_acc: 0.7176\n",
            "Epoch 11/100\n",
            "40000/40000 [==============================] - 37s 937us/step - loss: 0.8252 - acc: 0.7087 - val_loss: 0.7834 - val_acc: 0.7222\n",
            "Epoch 12/100\n",
            "40000/40000 [==============================] - 38s 945us/step - loss: 0.8136 - acc: 0.7137 - val_loss: 0.7576 - val_acc: 0.7343\n",
            "Epoch 13/100\n",
            "40000/40000 [==============================] - 38s 946us/step - loss: 0.7943 - acc: 0.7183 - val_loss: 0.7597 - val_acc: 0.7322\n",
            "Epoch 14/100\n",
            "40000/40000 [==============================] - 38s 951us/step - loss: 0.7745 - acc: 0.7264 - val_loss: 0.7196 - val_acc: 0.7456\n",
            "Epoch 15/100\n",
            "40000/40000 [==============================] - 37s 934us/step - loss: 0.7606 - acc: 0.7339 - val_loss: 0.7065 - val_acc: 0.7488\n",
            "Epoch 16/100\n",
            "40000/40000 [==============================] - 38s 940us/step - loss: 0.7487 - acc: 0.7363 - val_loss: 0.6913 - val_acc: 0.7551\n",
            "Epoch 17/100\n",
            "40000/40000 [==============================] - 37s 937us/step - loss: 0.7352 - acc: 0.7398 - val_loss: 0.7071 - val_acc: 0.7493\n",
            "Epoch 18/100\n",
            "40000/40000 [==============================] - 37s 934us/step - loss: 0.7223 - acc: 0.7470 - val_loss: 0.6856 - val_acc: 0.7611\n",
            "Epoch 19/100\n",
            "40000/40000 [==============================] - 37s 933us/step - loss: 0.7071 - acc: 0.7495 - val_loss: 0.6817 - val_acc: 0.7622\n",
            "Epoch 20/100\n",
            "40000/40000 [==============================] - 38s 939us/step - loss: 0.6959 - acc: 0.7544 - val_loss: 0.6676 - val_acc: 0.7641\n",
            "Epoch 21/100\n",
            "40000/40000 [==============================] - 37s 932us/step - loss: 0.6882 - acc: 0.7573 - val_loss: 0.6530 - val_acc: 0.7665\n",
            "Epoch 22/100\n",
            "40000/40000 [==============================] - 38s 939us/step - loss: 0.6745 - acc: 0.7631 - val_loss: 0.6468 - val_acc: 0.7732\n",
            "Epoch 23/100\n",
            "40000/40000 [==============================] - 37s 930us/step - loss: 0.6643 - acc: 0.7666 - val_loss: 0.6577 - val_acc: 0.7669\n",
            "Epoch 24/100\n",
            "40000/40000 [==============================] - 37s 930us/step - loss: 0.6528 - acc: 0.7698 - val_loss: 0.6267 - val_acc: 0.7782\n",
            "Epoch 25/100\n",
            "40000/40000 [==============================] - 40s 988us/step - loss: 0.6446 - acc: 0.7728 - val_loss: 0.6264 - val_acc: 0.7825\n",
            "Epoch 26/100\n",
            "40000/40000 [==============================] - 39s 973us/step - loss: 0.6345 - acc: 0.7759 - val_loss: 0.6305 - val_acc: 0.7793\n",
            "Epoch 27/100\n",
            "40000/40000 [==============================] - 37s 936us/step - loss: 0.6267 - acc: 0.7793 - val_loss: 0.6108 - val_acc: 0.7853\n",
            "Epoch 28/100\n",
            "40000/40000 [==============================] - 37s 934us/step - loss: 0.6168 - acc: 0.7825 - val_loss: 0.6142 - val_acc: 0.7816\n",
            "Epoch 29/100\n",
            "40000/40000 [==============================] - 38s 948us/step - loss: 0.6045 - acc: 0.7871 - val_loss: 0.6076 - val_acc: 0.7862\n",
            "Epoch 30/100\n",
            "40000/40000 [==============================] - 38s 943us/step - loss: 0.6018 - acc: 0.7861 - val_loss: 0.6095 - val_acc: 0.7872\n",
            "Epoch 31/100\n",
            "40000/40000 [==============================] - 38s 946us/step - loss: 0.5963 - acc: 0.7907 - val_loss: 0.5987 - val_acc: 0.7889\n",
            "Epoch 32/100\n",
            "40000/40000 [==============================] - 37s 937us/step - loss: 0.5842 - acc: 0.7935 - val_loss: 0.5959 - val_acc: 0.7909\n",
            "Epoch 33/100\n",
            "40000/40000 [==============================] - 36s 896us/step - loss: 0.5773 - acc: 0.7957 - val_loss: 0.5887 - val_acc: 0.7949\n",
            "Epoch 34/100\n",
            "40000/40000 [==============================] - 36s 889us/step - loss: 0.5700 - acc: 0.7999 - val_loss: 0.5770 - val_acc: 0.7962\n",
            "Epoch 35/100\n",
            "40000/40000 [==============================] - 35s 880us/step - loss: 0.5670 - acc: 0.7991 - val_loss: 0.5847 - val_acc: 0.7984\n",
            "Epoch 36/100\n",
            "40000/40000 [==============================] - 35s 884us/step - loss: 0.5517 - acc: 0.8031 - val_loss: 0.5742 - val_acc: 0.8007\n",
            "Epoch 37/100\n",
            "40000/40000 [==============================] - 36s 900us/step - loss: 0.5516 - acc: 0.8053 - val_loss: 0.5705 - val_acc: 0.7995\n",
            "Epoch 38/100\n",
            "40000/40000 [==============================] - 36s 889us/step - loss: 0.5469 - acc: 0.8047 - val_loss: 0.5719 - val_acc: 0.7973\n",
            "Epoch 39/100\n",
            "40000/40000 [==============================] - 35s 870us/step - loss: 0.5362 - acc: 0.8112 - val_loss: 0.5744 - val_acc: 0.7994\n",
            "Epoch 40/100\n",
            "40000/40000 [==============================] - 35s 873us/step - loss: 0.5303 - acc: 0.8124 - val_loss: 0.5997 - val_acc: 0.7921\n",
            "Epoch 41/100\n",
            "40000/40000 [==============================] - 35s 877us/step - loss: 0.5201 - acc: 0.8153 - val_loss: 0.5726 - val_acc: 0.8001\n",
            "Epoch 42/100\n",
            "40000/40000 [==============================] - 35s 872us/step - loss: 0.5229 - acc: 0.8152 - val_loss: 0.5615 - val_acc: 0.8031\n",
            "Epoch 43/100\n",
            "40000/40000 [==============================] - 36s 891us/step - loss: 0.5101 - acc: 0.8186 - val_loss: 0.5549 - val_acc: 0.8042\n",
            "Epoch 44/100\n",
            "40000/40000 [==============================] - 36s 891us/step - loss: 0.5016 - acc: 0.8219 - val_loss: 0.5569 - val_acc: 0.8026\n",
            "Epoch 45/100\n",
            "40000/40000 [==============================] - 36s 894us/step - loss: 0.5004 - acc: 0.8222 - val_loss: 0.5556 - val_acc: 0.8049\n",
            "Epoch 46/100\n",
            "40000/40000 [==============================] - 36s 896us/step - loss: 0.4943 - acc: 0.8245 - val_loss: 0.5649 - val_acc: 0.8021\n",
            "Epoch 47/100\n",
            "40000/40000 [==============================] - 36s 898us/step - loss: 0.4858 - acc: 0.8266 - val_loss: 0.5611 - val_acc: 0.8033\n",
            "Epoch 48/100\n",
            "40000/40000 [==============================] - 36s 902us/step - loss: 0.4826 - acc: 0.8293 - val_loss: 0.5334 - val_acc: 0.8120\n",
            "Epoch 49/100\n",
            "40000/40000 [==============================] - 36s 895us/step - loss: 0.4803 - acc: 0.8293 - val_loss: 0.5528 - val_acc: 0.8085\n",
            "Epoch 50/100\n",
            "40000/40000 [==============================] - 36s 890us/step - loss: 0.4705 - acc: 0.8301 - val_loss: 0.5476 - val_acc: 0.8106\n",
            "Epoch 51/100\n",
            "40000/40000 [==============================] - 36s 890us/step - loss: 0.4670 - acc: 0.8322 - val_loss: 0.5415 - val_acc: 0.8097\n",
            "Epoch 52/100\n",
            "40000/40000 [==============================] - 36s 895us/step - loss: 0.4616 - acc: 0.8366 - val_loss: 0.5404 - val_acc: 0.8065\n",
            "Epoch 53/100\n",
            "40000/40000 [==============================] - 36s 892us/step - loss: 0.4589 - acc: 0.8362 - val_loss: 0.5410 - val_acc: 0.8118\n",
            "Epoch 54/100\n",
            "40000/40000 [==============================] - 36s 891us/step - loss: 0.4493 - acc: 0.8394 - val_loss: 0.5404 - val_acc: 0.8120\n",
            "Epoch 55/100\n",
            "40000/40000 [==============================] - 36s 898us/step - loss: 0.4494 - acc: 0.8403 - val_loss: 0.5372 - val_acc: 0.8123\n",
            "Epoch 56/100\n",
            "40000/40000 [==============================] - 35s 884us/step - loss: 0.4449 - acc: 0.8421 - val_loss: 0.5381 - val_acc: 0.8099\n",
            "Epoch 57/100\n",
            "40000/40000 [==============================] - 35s 885us/step - loss: 0.4371 - acc: 0.8427 - val_loss: 0.5376 - val_acc: 0.8140\n",
            "Epoch 58/100\n",
            "40000/40000 [==============================] - 35s 882us/step - loss: 0.4340 - acc: 0.8455 - val_loss: 0.5380 - val_acc: 0.8134\n",
            "Epoch 59/100\n",
            "40000/40000 [==============================] - 35s 880us/step - loss: 0.4287 - acc: 0.8470 - val_loss: 0.5234 - val_acc: 0.8166\n",
            "Epoch 60/100\n",
            "40000/40000 [==============================] - 37s 919us/step - loss: 0.4233 - acc: 0.8474 - val_loss: 0.5261 - val_acc: 0.8160\n",
            "Epoch 61/100\n",
            "40000/40000 [==============================] - 37s 927us/step - loss: 0.4169 - acc: 0.8503 - val_loss: 0.5386 - val_acc: 0.8126\n",
            "Epoch 62/100\n",
            "40000/40000 [==============================] - 36s 898us/step - loss: 0.4175 - acc: 0.8502 - val_loss: 0.5333 - val_acc: 0.8150\n",
            "Epoch 63/100\n",
            "40000/40000 [==============================] - 35s 887us/step - loss: 0.4155 - acc: 0.8519 - val_loss: 0.5255 - val_acc: 0.8196\n",
            "Epoch 64/100\n",
            "40000/40000 [==============================] - 36s 894us/step - loss: 0.4103 - acc: 0.8539 - val_loss: 0.5427 - val_acc: 0.8124\n",
            "Epoch 65/100\n",
            "40000/40000 [==============================] - 36s 898us/step - loss: 0.4053 - acc: 0.8539 - val_loss: 0.5407 - val_acc: 0.8133\n",
            "Epoch 66/100\n",
            "40000/40000 [==============================] - 36s 893us/step - loss: 0.3991 - acc: 0.8582 - val_loss: 0.5322 - val_acc: 0.8191\n",
            "Epoch 67/100\n",
            "40000/40000 [==============================] - 36s 892us/step - loss: 0.3951 - acc: 0.8590 - val_loss: 0.5207 - val_acc: 0.8225\n",
            "Epoch 68/100\n",
            "40000/40000 [==============================] - 36s 898us/step - loss: 0.3917 - acc: 0.8589 - val_loss: 0.5413 - val_acc: 0.8159\n",
            "Epoch 69/100\n",
            "40000/40000 [==============================] - 36s 890us/step - loss: 0.3888 - acc: 0.8615 - val_loss: 0.5184 - val_acc: 0.8228\n",
            "Epoch 70/100\n",
            "40000/40000 [==============================] - 35s 884us/step - loss: 0.3804 - acc: 0.8663 - val_loss: 0.5330 - val_acc: 0.8175\n",
            "Epoch 71/100\n",
            "40000/40000 [==============================] - 35s 886us/step - loss: 0.3825 - acc: 0.8635 - val_loss: 0.5346 - val_acc: 0.8176\n",
            "Epoch 72/100\n",
            "40000/40000 [==============================] - 35s 877us/step - loss: 0.3770 - acc: 0.8633 - val_loss: 0.5216 - val_acc: 0.8204\n",
            "Epoch 73/100\n",
            "40000/40000 [==============================] - 36s 894us/step - loss: 0.3764 - acc: 0.8658 - val_loss: 0.5323 - val_acc: 0.8167\n",
            "Epoch 74/100\n",
            "40000/40000 [==============================] - 35s 883us/step - loss: 0.3738 - acc: 0.8667 - val_loss: 0.5327 - val_acc: 0.8192\n",
            "Epoch 75/100\n",
            "40000/40000 [==============================] - 35s 874us/step - loss: 0.3703 - acc: 0.8670 - val_loss: 0.5327 - val_acc: 0.8167\n",
            "Epoch 76/100\n",
            "40000/40000 [==============================] - 44s 1ms/step - loss: 0.3617 - acc: 0.8696 - val_loss: 0.5443 - val_acc: 0.8142\n",
            "Epoch 77/100\n",
            "40000/40000 [==============================] - 19s 476us/step - loss: 0.3611 - acc: 0.8720 - val_loss: 0.5341 - val_acc: 0.8203\n",
            "Epoch 78/100\n",
            "40000/40000 [==============================] - 19s 476us/step - loss: 0.3629 - acc: 0.8690 - val_loss: 0.5181 - val_acc: 0.8237\n",
            "Epoch 79/100\n",
            "40000/40000 [==============================] - 19s 478us/step - loss: 0.3520 - acc: 0.8740 - val_loss: 0.5525 - val_acc: 0.8160\n",
            "Epoch 80/100\n",
            "40000/40000 [==============================] - 19s 483us/step - loss: 0.3514 - acc: 0.8746 - val_loss: 0.5229 - val_acc: 0.8227\n",
            "Epoch 81/100\n",
            "40000/40000 [==============================] - 19s 480us/step - loss: 0.3450 - acc: 0.8776 - val_loss: 0.5400 - val_acc: 0.8204\n",
            "Epoch 82/100\n",
            "40000/40000 [==============================] - 19s 483us/step - loss: 0.3449 - acc: 0.8775 - val_loss: 0.5194 - val_acc: 0.8222\n",
            "Epoch 83/100\n",
            "40000/40000 [==============================] - 19s 482us/step - loss: 0.3421 - acc: 0.8772 - val_loss: 0.5379 - val_acc: 0.8209\n",
            "Epoch 84/100\n",
            "40000/40000 [==============================] - 19s 482us/step - loss: 0.3386 - acc: 0.8785 - val_loss: 0.5325 - val_acc: 0.8226\n",
            "Epoch 85/100\n",
            "40000/40000 [==============================] - 19s 485us/step - loss: 0.3336 - acc: 0.8794 - val_loss: 0.5417 - val_acc: 0.8155\n",
            "Epoch 86/100\n",
            "40000/40000 [==============================] - 19s 485us/step - loss: 0.3360 - acc: 0.8773 - val_loss: 0.5232 - val_acc: 0.8225\n",
            "Epoch 87/100\n",
            "40000/40000 [==============================] - 19s 483us/step - loss: 0.3305 - acc: 0.8801 - val_loss: 0.5402 - val_acc: 0.8211\n",
            "Epoch 88/100\n",
            "40000/40000 [==============================] - 19s 485us/step - loss: 0.3284 - acc: 0.8816 - val_loss: 0.5224 - val_acc: 0.8254\n",
            "Epoch 89/100\n",
            "40000/40000 [==============================] - 19s 481us/step - loss: 0.3263 - acc: 0.8836 - val_loss: 0.5337 - val_acc: 0.8210\n",
            "Epoch 90/100\n",
            "40000/40000 [==============================] - 19s 480us/step - loss: 0.3233 - acc: 0.8853 - val_loss: 0.5250 - val_acc: 0.8253\n",
            "Epoch 91/100\n",
            "40000/40000 [==============================] - 19s 481us/step - loss: 0.3253 - acc: 0.8847 - val_loss: 0.5231 - val_acc: 0.8245\n",
            "Epoch 92/100\n",
            "40000/40000 [==============================] - 19s 484us/step - loss: 0.3159 - acc: 0.8867 - val_loss: 0.5355 - val_acc: 0.8233\n",
            "Epoch 93/100\n",
            "40000/40000 [==============================] - 19s 485us/step - loss: 0.3152 - acc: 0.8858 - val_loss: 0.5313 - val_acc: 0.8259\n",
            "Epoch 94/100\n",
            "40000/40000 [==============================] - 19s 484us/step - loss: 0.3151 - acc: 0.8851 - val_loss: 0.5301 - val_acc: 0.8247\n",
            "Epoch 95/100\n",
            "40000/40000 [==============================] - 19s 487us/step - loss: 0.3140 - acc: 0.8867 - val_loss: 0.5444 - val_acc: 0.8203\n",
            "Epoch 96/100\n",
            "40000/40000 [==============================] - 19s 481us/step - loss: 0.3082 - acc: 0.8886 - val_loss: 0.5341 - val_acc: 0.8244\n",
            "Epoch 97/100\n",
            "40000/40000 [==============================] - 19s 481us/step - loss: 0.3036 - acc: 0.8904 - val_loss: 0.5393 - val_acc: 0.8214\n",
            "Epoch 98/100\n",
            "40000/40000 [==============================] - 19s 483us/step - loss: 0.3006 - acc: 0.8923 - val_loss: 0.5181 - val_acc: 0.8284\n",
            "Epoch 99/100\n",
            "40000/40000 [==============================] - 19s 480us/step - loss: 0.2997 - acc: 0.8917 - val_loss: 0.5543 - val_acc: 0.8182\n",
            "Epoch 100/100\n",
            "40000/40000 [==============================] - 19s 487us/step - loss: 0.3022 - acc: 0.8886 - val_loss: 0.5286 - val_acc: 0.8260\n",
            "Test loss is: 0.5286202352046967\n",
            "Test accuracy is: 0.826\n",
            "[0.5286202352046967, 0.826]\n",
            "{'val_loss': [0.9913422323226929, 0.9388770418167114, 0.9584608075141907, 0.9181338381767273, 0.8783048190116882, 0.8514315538406372, 0.8280164098739624, 0.7998393581390381, 0.7964684268951416, 0.7883220019340516, 0.7833678770065308, 0.7576291619300842, 0.7597259897232056, 0.7195522132873535, 0.706501835155487, 0.6912820086479187, 0.7071475896835328, 0.6856277352809906, 0.6816819643974305, 0.667649063539505, 0.6530122246742248, 0.646763223361969, 0.6576958448886872, 0.6267447489261627, 0.6263878743648529, 0.6304795378684998, 0.6107992186546326, 0.6141611632823945, 0.6075802473545074, 0.6095009807109832, 0.5987157888889313, 0.5959130990982056, 0.5886975006103515, 0.5770291013240815, 0.5846914491176606, 0.574233271408081, 0.5705005093574524, 0.5718777331352234, 0.5743541425704956, 0.5996831478118897, 0.572551791381836, 0.561515593624115, 0.5549317387104035, 0.5569479299545288, 0.5555977931976318, 0.5649241569519043, 0.5611316205978394, 0.5333720463275909, 0.5528107212543487, 0.5475666338920593, 0.5414701234817505, 0.5404026193618774, 0.5409591120243072, 0.5404130357742309, 0.5372128070831299, 0.5381148772239686, 0.5376421855926514, 0.5379644320964814, 0.5234078824520111, 0.5261350776672363, 0.5385608023643493, 0.533318793296814, 0.5255313164710999, 0.5426784626960754, 0.540674252653122, 0.5322315899848938, 0.5206544581890106, 0.5413227896213532, 0.518359900522232, 0.5330203733444214, 0.5346189487934112, 0.5215551878452301, 0.5322706932544709, 0.532664137840271, 0.5326934701919556, 0.5443381946086884, 0.5340689479351044, 0.518147266626358, 0.5524615071058273, 0.5229064511299133, 0.5400117372989655, 0.5193679702758789, 0.5379324382781983, 0.5324667033433914, 0.5417275796890259, 0.5232470296382904, 0.5402064139842987, 0.52238723487854, 0.5336517951250076, 0.5249761501789093, 0.5231070583820343, 0.5355206240653991, 0.5313408476829529, 0.5301015274524689, 0.5444031199455261, 0.5340721252918244, 0.5392991299629212, 0.5180943448543549, 0.5542558861732483, 0.5286202352046967], 'val_acc': [0.644, 0.6673, 0.6539, 0.6703, 0.6829, 0.6944, 0.7026, 0.7181, 0.7146, 0.7176, 0.7222, 0.7343, 0.7322, 0.7456, 0.7488, 0.7551, 0.7493, 0.7611, 0.7622, 0.7641, 0.7665, 0.7732, 0.7669, 0.7782, 0.7825, 0.7793, 0.7853, 0.7816, 0.7862, 0.7872, 0.7889, 0.7909, 0.7949, 0.7962, 0.7984, 0.8007, 0.7995, 0.7973, 0.7994, 0.7921, 0.8001, 0.8031, 0.8042, 0.8026, 0.8049, 0.8021, 0.8033, 0.812, 0.8085, 0.8106, 0.8097, 0.8065, 0.8118, 0.812, 0.8123, 0.8099, 0.814, 0.8134, 0.8166, 0.816, 0.8126, 0.815, 0.8196, 0.8124, 0.8133, 0.8191, 0.8225, 0.8159, 0.8228, 0.8175, 0.8176, 0.8204, 0.8167, 0.8192, 0.8167, 0.8142, 0.8203, 0.8237, 0.816, 0.8227, 0.8204, 0.8222, 0.8209, 0.8226, 0.8155, 0.8225, 0.8211, 0.8254, 0.821, 0.8253, 0.8245, 0.8233, 0.8259, 0.8247, 0.8203, 0.8244, 0.8214, 0.8284, 0.8182, 0.826], 'loss': [1.0856457291126251, 1.0424265901088714, 1.0146308158636093, 0.981919059753418, 0.9568848207473755, 0.9330088872909545, 0.9054831065893173, 0.8885678312778473, 0.8650863532543183, 0.8469020431518555, 0.8251993666648865, 0.8135559128522873, 0.7943340705871582, 0.7744789325237275, 0.7605877565145492, 0.7486782066822052, 0.735204903268814, 0.7223350804805756, 0.7070774590730667, 0.6959199694871903, 0.6882148341655732, 0.6744753417253494, 0.6642695464611054, 0.6527710116505623, 0.64456493486166, 0.6345282467484474, 0.6266727268099784, 0.6167992001414299, 0.6044954065799714, 0.601779005920887, 0.5963296332836151, 0.584188069498539, 0.5772993539214134, 0.5699824561357498, 0.5669851807594299, 0.5517203000545502, 0.5515593953251838, 0.5468968744397164, 0.5361914055347443, 0.5303329492807388, 0.5201408365428448, 0.5228983200550079, 0.5100961719036102, 0.5016202785372734, 0.5004236028909683, 0.4943223645687103, 0.48583216112852096, 0.48258557866215707, 0.4802521659612656, 0.47050046567320825, 0.46702026232481003, 0.4615767515182495, 0.45889964691400525, 0.44932810925245287, 0.44936861350536345, 0.4448905805647373, 0.4370771620869637, 0.4339919656693935, 0.4286978136062622, 0.42328501143455505, 0.416884685921669, 0.4175312982618809, 0.41547785641551016, 0.4103128006875515, 0.4052987484812737, 0.39905134680867194, 0.3950817880451679, 0.3917109621286392, 0.3888299268782139, 0.38041823167800903, 0.382455363458395, 0.3769820375919342, 0.37639265335202216, 0.3737737716794014, 0.37026118672788144, 0.3617059424042702, 0.3610610729455948, 0.36288127753436566, 0.35197443082332613, 0.3513824359536171, 0.34496695173978803, 0.34494092237353324, 0.3420780628323555, 0.33863725781440734, 0.3336105164796114, 0.3359823558628559, 0.33054067577719687, 0.3283851816892624, 0.3263179951965809, 0.32328204108178615, 0.32532745572924615, 0.3159272580444813, 0.31523922866880894, 0.3150653471827507, 0.3140252097606659, 0.30822294690907004, 0.30358185694515705, 0.3006036447763443, 0.2997444769948721, 0.30223611187040805], 'acc': [0.611, 0.63085, 0.639625, 0.6501, 0.659875, 0.669, 0.680025, 0.68635, 0.694225, 0.701125, 0.708675, 0.713725, 0.718325, 0.72635, 0.73395, 0.736275, 0.73975, 0.74695, 0.74945, 0.754425, 0.75735, 0.763075, 0.766625, 0.76975, 0.7728, 0.775925, 0.779325, 0.782525, 0.7871, 0.786075, 0.790725, 0.793475, 0.795675, 0.799925, 0.7991, 0.803075, 0.805275, 0.80475, 0.8112, 0.81235, 0.815325, 0.815225, 0.8186, 0.82195, 0.822175, 0.824525, 0.82655, 0.829275, 0.829275, 0.830075, 0.83225, 0.836625, 0.8362, 0.839375, 0.840325, 0.84215, 0.84265, 0.8455, 0.847025, 0.847375, 0.8503, 0.850225, 0.85195, 0.853925, 0.85395, 0.858225, 0.85905, 0.858925, 0.86145, 0.8663, 0.863525, 0.8633, 0.8658, 0.866675, 0.86695, 0.8696, 0.871975, 0.86905, 0.873975, 0.87455, 0.87765, 0.877525, 0.877175, 0.8785, 0.879375, 0.8773, 0.8801, 0.88155, 0.88355, 0.885275, 0.8847, 0.8867, 0.8858, 0.88505, 0.8867, 0.888625, 0.8904, 0.89225, 0.89165, 0.888575]}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3XlYlWX6wPHvw76IbKKoiIjigiIu\niJlm7lmmtpmalltZma3TNM5vmml3rJmmqamprDStxLVMS3M0NbVy30VUQEHEhR0FWQ7n+f3xHvEg\nKKgc1vtzXVye827nfqHe+zy70lojhBBCXItddQcghBCi5pNkIYQQolySLIQQQpRLkoUQQohySbIQ\nQghRLkkWQgghyiXJQgghRLkkWQghhCiXJAshhBDlcqjuACpLo0aNdFBQUHWHIYQQtcquXbtStdZ+\n5R1n02ShlBoKvA/YA59rrWddsb8lMAfwA9KB8VrrJMu+CcDLlkPf1FrPu9ZnBQUFsXPnzkq+AyGE\nqNuUUgkVOc5m1VBKKXvgI+BOIBQYq5QKveKwfwLztdadgdeBv1vO9QFeAXoCkcArSilvW8UqhBDi\n2mzZZhEJxGqt47XWBcBCYOQVx4QC6y2vN1jtvwNYq7VO11pnAGuBoTaMVQghxDXYMlk0B05avU+y\nbLO2D7jP8vpewEMp5VvBc4UQQlSR6m7gfhH4UCk1EdgEnAKKKnqyUmoqMBUgMDCw1P7CwkKSkpLI\ny8urlGBF1XBxcSEgIABHR8fqDkUIYWHLZHEKaGH1PsCyrZjWOhlLyUIp1QC4X2udqZQ6BfS74tyN\nV36A1no2MBsgIiKi1MIcSUlJeHh4EBQUhFLqpm5GVA2tNWlpaSQlJdGqVavqDkcIYWHLaqgdQIhS\nqpVSygkYA6ywPkAp1UgpdSmGP2P0jAJYAwxRSnlbGraHWLZdl7y8PHx9fSVR1CJKKXx9faU0KEQN\nY7NkobU2AdMxHvKHgcVa60NKqdeVUiMsh/UDjiiljgJNgLcs56YDb2AknB3A65Zt100SRe0jfzMh\nah6btllorVcBq67Y9jer10uBpVc5dw6XSxpCCFFv5eSb+GF/Mjn5RbT0daOlrxtBvu442FfdJBzV\n3cBdp6WlpTFw4EAAzpw5g729PX5+xkDJ7du34+TkVO41Jk2axIwZM2jXrt11ffbdd99NZmYmW7Zs\nuf7AhRA1QnzKBeb/nsCyXUmczzeV2Nfaz533x3SlU3PPKolFkoUN+fr6snfvXgBeffVVGjRowIsv\nvljiGK01Wmvs7Mr+hjB37tzr/tz09HT279+Pi4sLiYmJZfYUqwwmkwkHB/lPSIjKVGAyszb6LAu2\nJ/BrbBqO9oq7wprySK8gWvq6kZCWy7Gz53lv3VHu/e+vvDC4HVP7BmNvZ9vqW5lIsBrExsYSGhrK\nuHHj6NixI6dPn2bq1KlERETQsWNHXn/99eJj+/Tpw969ezGZTHh5eTFjxgzCw8Pp1asX586dK/P6\nS5cu5Z577mH06NEsXLiwePuZM2cYOXIknTt3Jjw8nG3btgFGQrq0bdKkSQCMHz+e5cuXF5/boEED\nANatW0e/fv24++67CQsLA2D48OF0796djh078vnnnxef8+OPP9KtWzfCw8MZMmQIZrOZNm3akJ5u\nND8VFRURHBxc/F6I+iztQj4f/HyM3m+v56kFuzmRmsuLQ9ry64wBvD+mK91betOogTPdW3ozJjKQ\nNc/1ZXBoE97+KYZH5mzDbC7VIbRS1Zuvha+tPER0cnalXjO0WUNeGd7xhs6NiYlh/vz5REREADBr\n1ix8fHwwmUz079+fBx54gNDQkrOjZGVlcfvttzNr1ixeeOEF5syZw4wZM0pdOyoqipkzZ+Lp6cm4\nceN46aWXAHjqqacYPHgw06dPx2QykZuby759+3j77bf57bff8PHxqdCDe+fOnURHRxeXWObNm4eP\njw+5ublERERw//33k5+fz5NPPsnmzZtp2bIl6enp2NnZMXbsWBYsWMD06dNZs2YNPXr0wMfH54Z+\nh0LUZFprfo9L4+eYc7Rq5E54gBft/D1wcrArcczuxEwW7Uhk+d5kCkxm+rXzY0KvIPq29btmacHL\nzYmPHurGt7tPkXmxEDsblyzqTbKoaVq3bl2cKMB4wH/xxReYTCaSk5OJjo4ulSxcXV258847Aeje\nvTubN28udd3k5GQSExPp1asXAGazmZiYGNq3b8/GjRuLSxoODg40bNiQ9evXM3r06OIHdkUe3L16\n9SpRtfXee++xYoXRKzopKYm4uDhOnjxJ//79admyZYnrTpkyhVGjRjF9+nTmzJnDo48+WrFfmBA1\nVF5hEWsOnWFPYibNvFxo4e1GnqmIL7Yc5+CpbOztFEWWb/1O9nYE+7nTzt8D/4YurD18lviUHFwd\n7XmgewCTewfRprFHhT9bKcX93QNsdWsl1JtkcaMlAFtxd3cvfn3s2DHef/99tm/fjpeXF+PHjy9z\nnIF1g7i9vT0mk6nUMYsWLSI1NZVL07VnZWURFRXFa6+9BlS8W6qDgwNmsxkwqousP8s69nXr1rFp\n0ya2bt2Kq6srffr0ueYYiaCgILy9vdmwYQN79uxhyJAhFYpHiJomPuUCX21N4Ls9p8jMLcTZwY58\nk7l4f3Ajd2beG8Z93ZqTcj6ffUmZHDiVxdEz59l5IoNTmReJDPLhib6tuatzUxo41+zHcc2Orp7I\nzs7Gw8ODhg0bcvr0adasWcPQoTc2b2JUVBTr1q2jR48egJGIhg0bxmuvvUb//v355JNPmD59OkVF\nReTk5DBgwABGjx7Ns88+W1wN5ePjQ1BQELt27eK+++7ju+++o6io7FlYsrKy8PHxwdXVlUOHDrFj\nxw4Abr31Vp599lkSEhKKq6GsSxfjxo1j0qRJV23YF6Km2nkinU83xbPu8Fkc7ewY0rEJYyMD6RXs\ny/l8EyfTc7lYWET3QO/iqqEWPm608HHj7s7Niq9jKjJXadfXmyXJogbo1q0boaGhtG/fnpYtW9K7\nd+8buk5cXBynT58uUb0VEhKCi4sLu3bt4sMPP+Sxxx7j008/xcHBgU8//ZTIyEheeukl+vbti4OD\nA927d+eLL77g8ccfZ+TIkfzwww/cfffdODs7l/mZw4YNY/bs2YSGhtKuXTt69uwJQJMmTfj4448Z\nOXIkWmuaNWvG6tWrAbj33nuZPHkyEydOvKH7FKKqxZ67wOoDp/nxwGlizpzHy82Rp/u34ZFbg2jU\n4PL/G56ujnhWsCtrbUoUAEpr27agV5WIiAh95eJHhw8fpkOHDtUUkbiarVu38uc//5kNGzZc9Rj5\n24mqUmAyc/TseaKTswnwcaVX8OUpgo6n5vCX7w7wW1waABEtvRnZpRn3dw/AzalufNdWSu3SWkeU\nd1zduFtRa7z11lvMnj27RJdeIWwh5kw2OflFODvY4exgh6erI15uTjjaK46evcC6w2dZH3OOA0lZ\nFBRdbmsID/DkyX5tiE+9wL/XHcPZwY7/u6s9I8Kb4+/pUo13VL2kZCFqJPnbiRt1Pq+QV1dEs2x3\nUpn7XR3tuVhotMF1DvDklmBfwpp7EtqsIdvi0/nklzgS03MBuLOTP6+N6EjjhnU3SUjJQghRrxhj\nFjJ4btFeTmVcZHr/NkQEeVNgMpNnMpN1sZCMnAIycgto07gBA9s3KVVSaO3XgAcjAlh3+CxuTg70\nbetXTXdT80iyEELUWqcyL/L26hhiz13gZEYu5/NMBHi7svjxXkQE3dhgTwd7O4Z2alrJkdZ+kiyE\nEDVeXmERJ9NzCWlyecBaZm4BE+Zs53TmRSJb+dAjyJugRu7c3z2Ahi6yymJlk2QhhKixisyab3cn\n8d7aoyRn5fFA9wD+encozg52PDZ/J4lpucyfEsktwb7VHWqdJ8nChipjinKAOXPmcNddd+Hv71/m\n/oKCAvz9/Zk2bRpvvvlm5QQvRDXbciyVN3+MJubMecIDPLmjkz/zf09gy7FUWjd2Z8eJDP4ztqsk\niioiycKGKjJFeUXMmTOHbt26XTVZrFmzhtDQUBYtWmTTZCFTkouqcCYrjzd+jObH/acJ9HHjo4e6\ncVeYP0op7u3anBeX7OPX2DReHtaB4eHNyr+gqBS1awhhHTJv3jwiIyPp0qUL06ZNw2w2YzKZePjh\nhwkLC6NTp0588MEHLFq0iL179zJ69Gi6dOlCQUFBqWtFRUXxwgsv4O/vz/bt24u3b9u2jV69ehEe\nHk7Pnj3Jzc3FZDLx/PPP06lTJzp37sx///tfAAICAsjMzASMQXODBg0C4OWXX+aRRx6hd+/eTJw4\nkbi4OG677Ta6du1K9+7di6c5B5g5cyZhYWGEh4fzl7/8hSNHjhRPOwJGd9jIyEib/D5FzZGQlsP5\nvMJyj8stMLHzRDpzfz3OaysP8YfF+3hs/k4GvruRddFneWFwW/73fF+GdW5aPEiuc4AXK5/uww9P\n9+HR24JtfSvCSv35mrh6Bpw5ULnX9A+DO2dd92kHDx7ku+++47fffsPBwYGpU6eycOFCWrduTWpq\nKgcOGHFmZmbi5eXFf/7zHz788EO6dOlS6lq5ubls3LiROXPmcObMGaKiooiMjCQvL48xY8awbNky\nunXrRlZWFs7Ozvz3v/8lOTmZffv2YW9vX6EpyWNiYti0aRMuLi7k5uaydu1aXFxciImJYcKECWzb\nto2VK1eyevVqtm/fjqura/FcUK6urhw8eJBOnToxd+7c4vUyRN30W2wqE+fuwNPNkVeHdywuEViL\nOZPNzFUxbDmWwqUlGNyd7PFyc8LDxYEBHZrwxyHtCPR1K/MznB3sq2x1OHFZ/UkWNci6devYsWNH\n8RxOFy9epEWLFtxxxx0cOXKEZ555hmHDhlVoRtYVK1YwePBgXFxcGDVqFN27d+fdd9/l8OHDBAYG\n0q1bNwA8PT2LP/u5557D3t4eqNiU5CNHjsTFxeiPnp+fz/Tp09m3bx8ODg7ExcUVX3fy5Mm4urqW\nuO6UKVOYO3cub7/9NkuWLGHPnj3X86sStcih5CymfrWLlr5uODva8dSC3Qzq0JiHegbSwNkRF0c7\nFu04SdT2RDxcHHmyX2u6tPAmrLknTRo6V3hGZFE96k+yuIESgK1orZk8eTJvvPFGqX379+9n9erV\nfPTRRyxbtozZs2df81pRUVFs3bq1eErylJQUfvnlF7y8vK4rJuspya+cYtx6SvJ3332XFi1a8PXX\nX1NYWFi8gt7VjBo1ipkzZ9K7d2969ep13XGJ2uFkei4T5+6goYsD86dE4tfAmbm/nuBfa4+y7vDl\nFR3t7RSP9AriuUEheLlVrIOHqBmkzaIaDBo0iMWLF5OamgoYvaYSExNJSUlBa82oUaN4/fXX2b17\nNwAeHh6cP3++1HUyMzPZunUrSUlJnDhxghMnTvDBBx8QFRVFaGgoiYmJxdfIzs6mqKiIwYMH88kn\nnxRPOX6pGurSlOQAy5Ytu2rsWVlZNG1q1CHPmzePS9PFDB48mDlz5nDx4sUS13Vzc2PAgAFMnz5d\nqqDqqM3HUnjo860UmMzMnxJJU09XHOzteKxvMFv+1J/vpt3K11N68sn47vz8wu28OqKjJIpaSJJF\nNQgLC+OVV15h0KBBdO7cmSFDhnD27FlOnjxJ37596dKlC5MmTWLmzJkATJo0iUcffbRUA/eyZcsY\nPHgwjo6XByDdc889LF++HDs7O6KionjyySeL18DOz8/n8ccfx9/fv3jN7cWLFwNGb61p06bRo0eP\na3bpnT59Op9//jnh4eEcP368eOryu+++m6FDhxIREUGXLl147733is8ZN24cjo6Oxd2IRe1UWGQm\nPuUC8SkXSMrI5fDpbB6dt5OHv9iOnVJ8OalHqVXefBs40zXQmz4hjRjayZ+gRu5Xubqo6WQiQWFz\ns2bNIj8/n1deeaXC58jfrmZIu5DPe+uOsu9kFkfOnqfAaiU4MBqmnx4YwqTeQTg72FdTlOJmyESC\nokYYPnw4J0+eZP369dUdirhO587nMe6zbSSk5xIZ5MPEW4No18QDB3tFfqEZs9YMaN+4Ts/IKi6T\nZCFsauXKldUdgrgBZ7LyeOizrZzJzmP+ZJlOQ9g4WSilhgLvA/bA51rrWVfsDwTmAV6WY2ZorVcp\npYKAw8ARy6FbtdZP3EgMWmvpklfL1JWq0dpCa81vcWn8fPgcJrNRYth0NJX0nALmT4684dlbRd1i\ns2ShlLIHPgIGA0nADqXUCq11tNVhLwOLtdYfK6VCgVVAkGVfnNa69Ci06+Di4kJaWhq+vr6SMGoJ\nrTVpaWnF4zqE7RSYzPx4IJnPNh0n+nQ2Lo52uDrao5TCy9WR+VMi6RboXd1hihrCliWLSCBWax0P\noJRaCIwErJOFBhpaXnsCyZUZQEBAAElJSaSkpFTmZYWNubi4EBAQUN1h1CmFRWYc7BRKKeJTLrBw\nx0mW7UoiLcdYCOjt+8MY2aU5Lo7SSC3KZstk0Rw4afU+Ceh5xTGvAv9TSj0NuAODrPa1UkrtAbKB\nl7XWm683AEdHR1q1anW9pwlRZySm5TLj2/38FpcGGIPiiswaBzvFwA6NGRsZSN8QP+zspOQtrq26\nG7jHAl9qrd9VSvUCvlJKdQJOA4Fa6zSlVHdguVKqo9Y62/pkpdRUYCpAYGBgVccuRI1lNmu+2prA\nrNUxONgpnuzXGid7O0xmM95uTozo0ozGHlLVJyrOlsniFNDC6n2AZZu1KcBQAK3170opF6CR1voc\nkG/ZvkspFQe0BUoMpNBazwZmgzHOwhY3IURNV2TWbD+ezsr9yRw5c56MnAJSL+STnWeib1s/Zt0X\nRjMv1+oOU9RytkwWO4AQpVQrjCQxBnjoimMSgYHAl0qpDoALkKKU8gPStdZFSqlgIASIt2GsQtQ6\neYVFfLwxjoU7EjmbnY+bkz2dAzzp0Kwhvu5OdG/pzYjwZtK5Q1QKmyULrbVJKTUdWIPRLXaO1vqQ\nUup1YKfWegXwB+AzpdTzGI3dE7XWWinVF3hdKVUImIEntNblz6UtRD2x/Xg6M5btJz41h4HtG/Py\nsOYM7NAYN6fqrlkWdVWdnu5DiLpAa83uxExiz50nOTOP2HMX+PHAaQK8XZl1X2f6hDSq7hBFLSbT\nfQhRy53PK+Tb3af4amsCsecuAKAUNPZwZnLvVvxhSFvcneV/YVE15L80IWoIs1mz9vBZtsWns+dk\nBoeSsykwmekc4Mk/HujMLcG++Hu64Ggvk0WLqifJQogaIOV8Pi8u2ccvR1NwdrCjc4AnE3q1ZFjn\nZnRpIQtGieonyUKIKpZXWMSaQ2dwdrCncUNnUs7n85fvDnI+r5A3RnZkTGSglB5EjSPJQogqlFdY\nxKPzdrIlNrXE9rZNGvDNoz1p5+9xlTOFqF6SLISoIpcSxa9xqbx1byfCA7xIOZ9PToGJge2b4Ook\n8zKJmkuShRBV4HxeIU9+vZtf41J55/7OjIpoUf5JQtQgkiyEsKHsvELmbjnBF1viOZ9vkkQhai1J\nFkLcpPN5hRxKvjzHZdbFQmJOn+fw6Wx+i0slO8/EoA5NeHZgCGEBntUYqRA3TpKFEDdIa82Kfcm8\n8cNhUi/kl9inFAT5ujOgfWMevS2YTs0lSYibtOML8G0Nwf2q5eMlWQhxA+JSLvDqikNsPpZK5wBP\nZt0Xhpuz0UDt5uRASOMGMrpaVJ7jm+HHF8C7FTy9G+ysulbnnwdH95LbbED+axbiOuw9mcknG+NY\nE30GdycHXhvRkfG3tMReFg+qH5L3QPpx6HRfye3nz8Kx/4FPK2jSCVyvGEhpLoLd82HXXPDrAK0H\nQOv+0KBx+Z9pyocfngcHF8g4DrFroe0dl/eveAZyUmDCSqNIayOSLISogP1Jmbzz0xG2xKbS0MWB\naf1aM/HWVvh5OFd3aPVPYR6ci4bm3ar2c49vggWjoTAXMk7AbS8Y2zMSYN7dkJl4+VivlhB8u5EU\n3Hxh7d+MRNOkk/Gw378QlB0MnQU9H7983oVzsPJZCOwFvZ4CO3vY8m9IOwZjF8EPz8G2Ty8ni5Pb\n4dC3cPufbJooQJKFEKXkFRZxNjuPC/kmsi4W8s22RH7cfxpvN0f+7672PNSzJQ3qYxVTQa7xYOow\nAlwaXt5eZIL1b0D7YdAi0rYxpB+HxY/Amf0wJgra33V5X142bH63dBxaG8c3DAB338vbs5Ph4LcQ\n1BuadS35OQm/Q1YShAw2SglxGyBqLHgHQeP28PNrxsM+dCTMGw752fDwcqMEcfYAJO2EQ98bpQkA\nj6Zw/xfQ6X5LPPvgl3dg9UuAgp5TjWQzf6SRiI6sMn5uexE2/9M4r91QODMZNrwFKUehUQis+T9o\n4A+3PmOjX/hlMkW5EFZ+jU3l2YV7SL1QULzN1dGex25rxWN9g/FwcazG6KpRejwsesR4EHYeA/d9\nennfbx/C//4CLl7w2HqjEbYsl541N/oN+Oj/4NtHjZVvXBqCvRNM2woOTsb+lc/Cri+N123vhD7P\nw6ldsHOO8c0cZSSR4P7Gt/zYtaDN4BUI07aBk5txbvZp+LAHFJwHOwdo2RsStxoP50e+N+7zu8fh\n4FJw8TSSxiPfQ9PwkvEWmSB5N6Qeg9AR4HzF6HxTASydBDE/GElhXxQUXICHlhi/79UvGUnI2ROm\n7wCPJkbJ472O0G0CtOwFSyfDiA+h28M39jul4lOUS7IQAmPG1/9ujOVfa48S7NeAqX2DaejigLuz\nA6FNG+LboA5WN50/C6f3Qdsh1z7uyGr49nHjIR98O0R/b1SJtBtqfAv+by/jm/m5aHBvDI+uNR6i\n1hK3GvXu7n4w6ktw87m8z1QAuggcr7H0676FxgO6SRiM/sp4AC8YBXf8HXpNM6qI5g2HyMeNh+qW\n9yE/yzi3eYTxMM0+DUd/gtN7jW/jXceBX3v49jHo8wIMesU4fvEjcHQN3P85JO2AwyvB1RvGLb0c\nd5EJvp8G8Rth/DLwD7uOX7wVUwEsmQhHfjR+Nw9/d/laWUmw7lXoMNwowVzy3ROXY3Lxgsd/Maqr\nbpAkCyEqKDO3gOcX7WXDkRRGdmnGzHvD6n5PpvNnYe5Q4xvs+GXQZlDZx0V/bzw8m4bDg18Z1Smz\n+8HFdJj2Oyx71EgET20zrvXVvUY9/diFxvl5WUa1yY7PoWFzyEmFhs1g3BLwaW3U3a97zejRc9sL\nRj39lUkjNRY+7WskpHFLjBKA1vD1fUbJ4YlfjTYDZWe8dnKDixlwaLnRrnHlN/6LGeDkAfaWv/Hy\nabB/ETyxxagKWvAgDPgr9H2x/N9jkenydW6UqQC2zzaq1HyCyz8+eY/xNwCjRBPc76Y+XpKFEBWw\nPymTJ7/eTcr5fP46PJTxPQNr75rVaXHGN32fVuDZAuyvUmWWmw5fDjMaZt18jOOe/B0cXUoelx4P\nn95uVL9M/PHyQzx5D3w20PhWfu4QDH0bbnnC2LfjC6OLJwqjvgjj9S1PQv+/wNlDsPAhMBca9f+n\n90Hz7sY3/SM/gmcgDH4NOt5rlGRMBfDFYMhMMBKBZ/PL8Z2Nhk96g1sjyDkHE1cZ7Q/XKycVPoyA\nRu2MdgwnN3h88+XqrZro6/uNaq1RX970pWSlPCGuUGAys3JfMhfyTTja25F6IZ8P18fi5+HMkid6\nEV5b143QGrZ9YvS4KbK0tSh7o8ronk+MaplL8rKMb+Rpcca3dF1klAZ+fR/6/enycYV5sHiC8W39\ngbklv+036wp9njMak5t1g8jHLu/rMQWcGkDqUaNNwd7R6CJ6qQE5sKfRrhE1Bi6kwL2zIWyUMUbg\n+Cb46c9GPf722TD070bp4PReGP11yUQB0CQUuk802iQiptxYogBwbwSDXoOVlkbiST/V7EQBRmmw\niknJQtQLR86c54XFe0tMywFwe1s//j26C97uNfzhcInWEL3cqEbyaQUe/rBhplEX3/ZO4xt81knj\nYb39s8vfPlv0hL3fGMfmpMDob4w2B4Alk4yeN9O2GtcE+OEF2PmFUZ3U7s7ScZjyYdM/IXzM1Ru0\nr6XIZPx7ZRXOpfEI69+E3DRAGwlh+PtlX+diJuz5CrpPAucG1x9H8eeaYdlk8G0DA16+8evUQlIN\nJQRwLjuPJbuSeH/dMTxcHHjznk70DPalsMiMWWv8G7rcfLWT2Wx8Q79atU+FrlEEBTlGb5i8LKNe\nvSDH6L1zqbFYa6P08NsHJc+1d4Ihb0Lk1JI9jc4egkXjjeom7yBIj4OAHnDHzJJdS7OTjd4/jTsY\nI4ST9xi9h2592rhudcjLMkouKUfhgS/Ayb164qgHJFmIeutCvon31x1l45EUjp27AMDQjv68dW8n\n2/RqWvWS0Y3ygTklGxu1hrxMo9fK1SRuhUUPG3XuZXHzNQZcdZtgdKXcPQ96PAZ9/2i0T2ScgGZd\nwK9d2efnZRkjfM8dhv7/Z/SqKSs5bvvUuL5HU6PKqOWt0POJm0uAolaQZCHqrT8t3c+SXSfpE+JH\n79a+9AlpRGjThrZpuM44Af/pbrQRmAth8BtGVVD097D5X8a4hEsP4MBeRv3+pfr/zESY3d+oKgof\na3x7dnIzukO6ehnJZst7cGIzODc0+tzf9qJRTWKLe7mYWXqaClHnSQO3qJfWx5xl0c6TTOvXmpeG\ntrf9B276h5EonvwNfn7VGJy25V9GfbtvCPT7P0iLNQZnHVlljBd4YA54BkDUQ1BUCA8tBr+2ZV+/\n9QA4ttaokgkdYXQttRVJFOIaJFmIOiMjp4A/LTtAe38Pnh0UYvsPTD8Oe6OM0kKjNsY4hF//bYw0\n7jnVmBbDerDUsbXGgKrZ/aBJR6Pb6UNLrp4owChBtB1S/sA5IWxMkoWotVIv5LM1Po1GDZxp7uXK\nO2uOkJFTwJeTeuDsUEnrWZuLjO6jZVX7bPqnUaff53njvVLG60vvrxQyGJ78Fb6dCsd/MRqaQ64y\nGE6IGsamyUIpNRR4H7AHPtdaz7pifyAwD/CyHDNDa73Ksu/PwBSgCHhGa73GlrGK2iP1Qj6zN8Uz\n//cT5BWaS+x7YXBbOja7joWGctIg4Vc4uc3oMdTpfmOgWuFFo6//lveM3kjD3y/ZeJ0eb8zlEznV\n6L5aUR7+xoRzqUev3igtRA1GXzeyAAAgAElEQVRks2ShlLIHPgIGA0nADqXUCq11tNVhLwOLtdYf\nK6VCgVVAkOX1GKAj0AxYp5Rqq7UuslW8onaI2p7I6yujyTcVMbJLcx7u1ZLc/CJOZeaSV2jmoZ6B\nFbtQ8h748UU4ZekUYedoNFD/9GdoM9AYWXz+tNFmkHHCmA20y3hjbEL8Rjjyk6VU8dz134SdnTFz\nqRC1iC1LFpFArNY6HkAptRAYCVgnCw1cmuvYE0i2vB4JLNRa5wPHlVKxluv9bsN4RQ2mteaDn2N5\nf10MdwY78sK9t9Pa7wYGYRUVGr2UNr1jTNw24GUI6mvMIZQSYzRAH1pujDe4/wtjVHDhRWM66V/f\nh71fG6uStboNIiZfX6lCiFrMlsmiOXDS6n0S0POKY14F/qeUehpwBy5V4DYHtl5x7hVj/UVdtulo\nCpuPpdC2iQcdm3myaPsJUrYv4feGy2mSnAhRwca3/pA7jEnwrJeUNBcZPZCurOYpyIF5I4zSRNgo\nuOsfJcdA+IcZP3e8VfI8R1djRtIu4+DCWWNgW02fDkKISlbdDdxjgS+11u8qpXoBXymlOlX0ZKXU\nVGAqQGBgBasfRI33/d5TPL9oLxqjRNHPbh9/cFhMmNMJtGd76PSyMXX03gXGbKaNQ42Ba+3uhANL\njHaGtFi491NjOopLfv+vkSjunQ3ho68/sEZtjB8h6iFbJotTQAur9wGWbdamAEMBtNa/K6VcgEYV\nPBet9WxgNhiD8iotclFtFu88yZ+W7adnkDdz+mRh98vbuJzbw0X3ABjyKSps1OXuqKZ8iF4Bv7wN\nSyaAgyuYLhqlg8Yd4X9/NRKIi6cxad2v/4b2d99YohCinrNlstgBhCilWmE86McAD11xTCIwEPhS\nKdUBcAFSgBXAAqXUvzAauEOA7TaMVVSziwVFzPnlKFs2rOQj3xiG5u/FbkmcMdX28PdxDX+odNWP\ngzN0HgWd7oODyyB2HXR6wOiienqfMZ5hw9/hzllGG0XhRRj0ajXcnRC1n82ShdbapJSaDqzB6BY7\nR2t9SCn1OrBTa70C+APwmVLqeYzG7onamH/kkFJqMUZjuAl4SnpC1U15hUV8vTWBxRt388/CN3nK\n6Tj6ohPKv6/R06jzmPLbB+zsofODxs8lzboYDdDbPzWWn9w5x5i9tFEVDNYTog6SuaFEtfn58Fn+\n9v0hVFYCS93fwU+nYz/8PWPk881MN31Jbroxb9PFDHB0g2f3QoPGN39dIeqQis4NZVfeAUJUtnPZ\neUz7ZhdT5u0kzP4E671m4u+Qi/3EldDlocpJFGAMrhv8GqCh9zOSKIS4CdXdG0rUMydScxj16e8E\nXYzm54D1BKduRHk0hYdXGOspVLauDxsL2gREln+sEOKqJFmIKnMmJZVvPn2Pz03/I9zhKFzwgtv+\nYEzp7d7INh+qlLE2gxDipkiyEDZRZNbM3RKHZ85xejrE4pe+i4aHV/IX8sjzbA29Zhnf+iurykkI\nYVOSLMTN0doYIOffGRxdADCbNbMW/MT4o8/Q0s5YAS5dN2CD7kmHYdMJjRxsm8V7hBA2I8lC3Jzd\n82Dls8YUGGMWoN39+MfSDTx89Gn8nArI7v8v9qgO/J7pzcDQJoQG+VR3xEKIGyDJQlSMKR+Obzam\n6ba3/GdzLgZWzzBGTJ89hP6sP9/4/4n7D/+dJg45OE36Edfm3bgduL0aQxdC3DxJFqJ8hXmwaJwx\nQrpFT7j/c2PG1qWTjXWjxy0l82wiOmo04488TYG9M46PfIdq3q26IxdCVJJyx1kopZ5WSnmXd5yo\nowpyIWoMxP4MPR6Fs9HwSR9YNN5YFvTeT/ntnANDFmUzIv8N4v3vxHH8YlRQ7+qOXAhRiSpSsmiC\nsXDRbmAOsEbXlWHf4toKciFqtFH9NPIj6DoOej0FS6dA7Dr0LdP5MLEl/1q3jVaN3Plw0giCm42v\n7qiFEDZQoek+lFIKGAJMAiKAxcAXWus424ZXcTLdhw1s+Tese6X0VN+mAnJi1vPcDk/WHknnni7N\nmHlfGG5OUqspRG1T0ek+KvR/t9ZaK6XOAGcwJvbzBpYqpdZqrV+6uVBFjaQ17J4PgbeWTBTA3tO5\nPL3KiTNZGbw+siMP39ISJV1hhajTyk0WSqlngUeAVOBz4I9a60KllB1wDJBkURcl/AbpcdD3xeJN\nRWbNxxtjeW/dMZp4OLPo8V50C5TmLCHqg4qULHyA+7TWCdYbtdZmpdTdtglLVKnoFXDoO7jn4+KB\ndez5Cpw8IHQkAKcyL/LCor1sO57O3Z2b8ta9YXi6OlZj0EKIqlSRZLEaSL/0RinVEOigtd6mtT5s\ns8hE1cg6BcunQcF5aNjMWH86LwsOLYfw0WhHN5bvSeJvyw9h1pp3R4VzX7fmUu0kRD1TkWTxMWDd\nYf5CGdtEbaQ1/PA8mE3QYTj8/hG0HQppx8B0kfOhY5mxYA8/HjhNREtv3hvdhRY+btUdtRCiGlQk\nWSjrrrKW6ifp9lIXHFgCx9bAHX+H7hPg7G2w/Elw8SLHqx2DF14gNaeAP97Rjidub429nZQmhKiv\nKrL4UbxS6hmllKPl51kg3taBCRu7cA5Wv2Ss89DzcWMk9n2z0dnJcPYA/0yNxM3Zge+m9eap/m0k\nUQhRz1UkWTwB3AqcApKAnsBUWwYlbEhriP4e5twBBTkw8kNjDWsg1SuMRe7jyNZu2HV+kB+e6UNY\ngGc1ByyEqAnKrU7SWp8DxpR3nKjhtIbjv8D6N40pxf06wLgl4NcOgIOnspg6fyfpuXfhfs8L/LV7\nq2oOWAhRk1RknIULMAXoCLhc2q61nmzDuERlKcg12ia2fQLnoqGBP4z4D3QZV1yi2BBzjie+3oWv\nuxNLn7iVTs2lNCGEKKkiDdVfATHAHcDrwDhAuszWZGlxsH8xnNhilCKK8qFJJxjxIYSNujyWAkhI\ny+GZqD20adyAeZMjadTAuRoDF0LUVBVJFm201qOUUiO11vOUUguAzbYOTNwgcxHMHwnZp4zV6yIf\ng3Z3QsvepVanyyss4smvd2Nnp/hkfHdJFEKIq6pIsii0/JuplOqEMT9UY9uFJG7K8V8g6yQ8MAc6\n3X/NQ19beYjo09nMmRgh4yeEENdUkWQx27KexcvACqAB8FebRiVu3N4ocPGEdsOueojZrPliy3Gi\ntp9kWr/WDGjfpAoDFELURtdMFpbJArO11hnAJiC4SqISFWPKBwerqqO8bDi80pgl1qpdwtrJ9Fz+\ntGw/v8WlMahDE14Y3LaKghVC1GbXHGehtTYjs8rWTNnJ8F4nWPns5W3R34PpInR5qMxTVu5LZui/\nN7E/KYu/3xfGZ490x8G+IkNthBD1XUWeFOuUUi8qpVoopXwu/VTk4kqpoUqpI0qpWKXUjDL2v6eU\n2mv5OaqUyrTaV2S1b8V13FPdZzYbk//lnINdX8Keb4zt+6LApzUE9Ch1ysYj53hu0V5CmzVkzfN9\nGRsZKJMBCiEqrCJtFqMt/z5ltU1TTpWUUsoe+AgYjDHye4dSaoXWOrr4Ilo/b3X800BXq0tc1Fp3\nqUB89c+OzyB+A9z1T6M08eMfwM0HEn6FAS+X6vV08FQWT32zm3ZNPJg7KZIGzjK1lxDi+lRkBPeN\nDuWNBGK11vEASqmFwEgg+irHjwVeucHPqj9SjsLav0HIEOjxKHQYAZ/0gYXjAAWdSw62T8rIZdKX\nO/Byc2LupB6SKIQQN6QiI7gfKWu71np+Oac2B05avb80r1RZn9ESaAWst9rsopTaibGM6yyt9fIy\nzpuKZZ6qwMDAcsKpAy6kwLIp4OhmDLBTCjyawP2fw1f3QKvbwKsFYKxqt3jnSf655ggFRWa+ebQn\nTRqW3egthBDlqcjXTOsKcBdgILAbKC9ZXI8xwFKtdZHVtpZa61NKqWBgvVLqgNY6zvokrfVsYDZA\nRESEpi6L3wjfToWLmfDgfCNJXBJ8O0xYCV4tAdiVkM5fvjtIzJnz9Ajy5rURnWjbxKN64hZC1AkV\nqYZ62vq9UsoLWFiBa58CWli9D7BsK8sYSraJoLU+Zfk3Xim1EaM9I670qXWc1rBhJmz6BzQKgfHL\nwD+s9HFBfQA4dz6PiXN30NDFkY8e6sZdYf7SkC2EuGk30m8yB6PKqDw7gBClVCullBNGQijVq0kp\n1R7wBn632uatlHK2vG4E9ObqbR1126ldsOkd6PwgTN1YdqKw8vdVMeQXmvlqSiTDOjeVRCGEqBQV\nabNYidH7CYzkEgosLu88rbVJKTUdWAPYA3O01oeUUq8DO7XWlxLHGGCh9Wp8QAfgU6WU2fKZs6x7\nUdUr0cvBzhHufMdYoOgafo9L47s9p3h6QBuC/RpUUYBCiPqgIm0W/7R6bQIStNZJFbm41noVsOqK\nbX+74v2rZZz3G3Dtr9D1waWFioL7gavXNQ8tMJn52/cHCfB2ZVq/NlUSnhCi/qhIskgETmut8wCU\nUq5KqSCt9QmbRibg9D7ITIS+5Q+i/2LLcY6du8AXEyJwdbKvguCEEPVJRdoslgBmq/dFlm3C1qK/\nB2UP7a8+KaDWmk9+ieMfa2IYHNqEgR1kUkAhROWrSMnCQWtdcOmN1rrA0mAtbOlSFVSr24zR2WXI\nyTfx0rL9/Lj/NMPCmvLOA52rOEghRH1RkWSRopQacalBWik1Eki1bViCc9GQHge3Ti+1S2vNz4fP\nMXP1YU6k5jDjzvY83jdYej4JIWymIsniCeAbpdSHlvdJQJmjusVNMOXD3gUQMhg8A4xSBQra313i\nsF0J6cxcFcOuhAxaNXJn/uSe9AlpVD0xCyHqjYoMyosDblFKNbC8v2DzqOqjvd/AD88b3WTDx0DC\nb8ZSqA0uL0oYcyabBz/dSqMGTvz9vjAe6B6Ao0wxLoSoAuU+aZRSM5VSXlrrC1rrC5YBc29WRXD1\nyv7F4NsGuk80XqfHQeiI4t1aa974IRoPFwd+etaYYlwShRCiqlTkaXOn1rp4nQnLqnl32S6keijj\nBCT+bixaNOyf8NwBGP4+dJtQfMja6LP8GpvG84Pa4u0u/QuEEFWrIm0W9kopZ611PhjjLADncs4R\n12O/pSdy2IPGvx5NjBKGRb6piLdWHSakcQPG9awHs+sKIWqciiSLb4CflVJzAQVMBObZMqh6RWvY\nvxBa9imeXvxKc389QUJaLvMnR8oyqEKIalGRBu63lVL7gEEYc0StAVraOrB6I3k3pMVC72fL3J1y\nPp8P18cysH1j+rb1q+LghBDCUNGvqWcxEsUoYABw2GYR1Tf7FoG9s7HiXRk++PkYFwuL+L9hHao4\nMCGEuOyqJQulVFuMpU7HYgzCWwQorXX/Koqt7isqhIPLoN2dZU4UGJ9ygajtiYyNbEFrmUVWCFGN\nrlUNFQNsBu7WWscCKKWer5Ko6otdX0JuKnQeXebuf6w5gpODHc8ObFu1cQkhxBWuVQ11H3Aa2KCU\n+kwpNRCjgVtUhoPLYNUfofUACBlSavfuxAxWHzzD1L7B+HlI5zMhRPW6arLQWi/XWo8B2gMbgOeA\nxkqpj5VSpZ9uoiSzGcxFZe87stpYTzuwF4z+BuxLFvC01vx91WEaNXDmsduCqyBYIYS4tnIbuLXW\nOVrrBVrr4RjraO8B/mTzyGq7ze/CP1rDsXUltx9YCosngH9neGgROLmVOvWDn2PZcSKD5waF4O5c\nkd7NQghhW9fVaV9rnaG1nq21HmirgOqMg0vhYgZ88wBs/hfkpMGSibBsCjQNh/HLwKVhqdM+3xzP\ne+uOcn+3AB6KlAF4QoiaQb622kLmSUiJgf4vQ8ph+Pk1+OVto1pqwF+h93Olqp4AFmxL5M0fD3NX\nmD9v3x+GnZ00EQkhagZJFrYQa6l6Ch0BjV6EZt3g2BoY8hY0LXuBolUHTvOX5Qfo386Pf4/uKiO1\nhRA1iiQLW4hdB56B0KgtKGUsYFTGIkaX7ErI4LlFe+kW6M3H47vj5CCJQghRs8hTqbKZCiD+F2gz\n0EgU5UhIy+Gx+Ttp5unCZ49E4OJoXwVBCiHE9ZFkUdmStkPBeWgzqNxDs3ILmTR3B1pr5k6KxEem\nHhdC1FBSDVXZYteBnQO06lvuoW+viSEhPZeFU2+hVSP3KghOCCFujJQsKtuxddDiljK7xVo7lJxF\n1PZEHunVkh5BPlUUnBBC3BhJFpUp+zScPQAh166C0lrz2spovN2ceE7mfRJC1AI2TRZKqaFKqSNK\nqVil1Iwy9r+nlNpr+TmqlMq02jdBKXXM8jPhynNrpLj1xr/ltFesOnCG7cfT+cOQtni6OVZBYEII\ncXNs1mahlLIHPgIGA0nADqXUCq119KVjtNbPWx3/NNDV8toHeAWIwFhHY5fl3AxbxVspDiyBBv7Q\npNNVD8krLGLmqsO09/dgTA8ZoS2EqB1sWbKIBGK11vFa6wJgITDyGsePBaIsr+8A1mqt0y0JYi0w\n1Iax3rzjmyB+A/R66ppdZv+7IZZTmRd5ZXhH7GWEthCilrBlsmgOnLR6n2TZVopSqiXQClh/Pecq\npaYqpXYqpXampKRUStA3RGtY9yo0bA6RU6962NGz5/n4lzju7dqcXq19qy4+IYS4STWlgXsMsFRr\nfZU5vctmmdQwQmsd4edXjetTH14Jp3ZBvz+Do0uZh5jNmj9/ewB3ZwdeliVShRC1jC2TxSmghdX7\nAMu2sozhchXU9Z5bvYpMsP4NaNQOwsde9bBvtieyKyGDl4eF4ttAFjMSQtQutkwWO4AQpVQrpZQT\nRkJYceVBSqn2gDfwu9XmNcAQpZS3UsobGGLZVvPsWwCpR2HgX8ucSRbgTFYe76yOoXcbX+7vVmZN\nnBBC1Gg26w2ltTYppaZjPOTtgTla60NKqdeBnVrrS4ljDLBQa62tzk1XSr2BkXAAXtdap9sq1hum\nNfz2H2jWFdrffdXDZq0+TH6RmbfuCUNVYL4oIYSoaWw63YfWehWw6optf7vi/atXOXcOMMdmwVWG\npB1GqWLEh1ftAbU/KZPle5OZ1q81QTKlhxCilqopDdy1056vwdENOt5T5m6tNW/9eBhfdyee7Ne6\nioMTQojKI8niRhXkwMFvIfQecPYo85B1h8+x7Xg6zw1ui4eLjNQWQtRekixu1OGVxlTkXceVubuw\nyMzfVx2mtZ87Y3u0KPMYIYSoLSRZ3Kg9X4N3K2jZu8zdX29NID41h/+7q4MskSqEqPXkKXYj0o/D\nic3QZVyZDdvHU3N456cj3N7WjwHtG1dDgEIIUbkkWdyIfVGAgi6lB+GZisw8v2gvTg52vPNAZ+kq\nK4SoE2SlvOulNexbCMG3g2dAqd0fb4xj78lM/jO2K00alj31hxBC1DZSsrheSTshMwHCHiy160BS\nFu//fIwR4c0YHt6sGoITQgjbkGRxvQ4uBXtn6FB6xPbbP8Xg7e7E6yM7VkNgQghhO5Isroe5CA59\nByGDwcWzxK6EtBy2xKbyyC0t8XJzqqYAhRDCNiRZXI8Tm+HCWQh7oNSuhTtOYqdgVISMqRBC1D2S\nLK7HgaXg1ADally0r7DIzJKdSQxo3wR/T2nUFkLUPZIsKsqUD4dXQPth4OhaYtfPh8+SeiGfh3pK\nqUIIUTdJsqio2J8hLws6la6CWrD9JE09Xbi9rQzAE0LUTZIsKurgUnD1gdb9S2w+mZ7L5mMpPBjR\nAns7GYAnhKibJFlURJEJjv7P6C5rX3L22MU7TwLwoEwWKISowyRZXOnEr3Axo+S2U7uMGWZbDyyx\nObfAxNdbExjYvjHNvUq2YwghRF0iycJaXhbMGw7r3yy5PX4joKBV3xKbF2xLJCO3kCf7tamyEIUQ\nojpIsrB25iDoImOtCrP58vb4jdA0HNx8ijflm4r4bHM8twT70L2ld9XHKoQQVUiShbUzB4x/L5yF\nk9uM1/kXIGk7BPcrceiyXac4m53P9P4hVRqiEEJUB0kW1s4cABcvY+6n6O+NbQm/gdlUoheUqcjM\nJ7/EEd7Ci95tfKspWCGEqDqSLKyd2Q/Nu0HrAcYAPLMZ4jeAgwu0uKX4sJX7k0lMz2V6/zayXoUQ\nol6QZHGJqQBSYsA/DEJHQvYpSN5ttFcE3gKOxjQeWms+2RhPuyYeDJRV8IQQ9YQki0tSj0JRAfh3\nhnZDwc4Rtn8G56JLtFdsOpbKkbPnmdo3GDsZhCeEqCckWVxyqXHbPwxcvY2V8PYvNLYFX26v+Hxz\nPI09nGVxIyFEvSLJ4pIzB8DBFXwtYyZCRxr/unobpQ0g5kw2m4+lMuHWIJwc5FcnhKg/bPrEU0oN\nVUodUUrFKqVmXOWYB5VS0UqpQ0qpBVbbi5RSey0/K2wZJ2A0bjcJBTt74327YaDsodXtYGf8mj7f\nfBxXR3vG9Qy0eThCCFGTONjqwkope+AjYDCQBOxQSq3QWkdbHRMC/BnorbXOUEpZtxhf1Fp3sVV8\nJWhtlCw63nN5m7svPDgfGncA4Fx2Ht/vPcXYyEBZCU8IUe/YLFkAkUCs1joeQCm1EBgJRFsd8xjw\nkdY6A0Brfc6G8VxdVhLkZRrtFdas1tme/3sCJrNmcu9WVRycEEJUP1tWQzUHTlq9T7Jss9YWaKuU\n+lUptVUpZb0EnYtSaqdl+z3YUnHjdniZu/MKi/hmWwKDOzQhqJG7TUMRQoiayJYli4p+fgjQDwgA\nNimlwrTWmUBLrfUppVQwsF4pdUBrHWd9slJqKjAVIDDwJtoRzhwAlNFmUYYf9p8mI7eQibcG3fhn\nCCFELWbLksUpwHqRhwDLNmtJwAqtdaHW+jhwFCN5oLU+Zfk3HtgIdL3yA7TWs7XWEVrrCD8/vxuP\n9Mx+oxeUU9mlhq9+P0Gbxg3o1Vqm9hBC1E+2TBY7gBClVCullBMwBriyV9NyjFIFSqlGGNVS8Uop\nb6WUs9X23pRs66hcZw6Ubq+w2Hcyk31JWTx8S0uZ2kMIUW/ZLFlorU3AdGANcBhYrLU+pJR6XSk1\nwnLYGiBNKRUNbAD+qLVOAzoAO5VS+yzbZ1n3oqpUFzMhM+GqyWL+7wm4Odlzb7crm1uEEKL+sGmb\nhdZ6FbDqim1/s3qtgRcsP9bH/AaU/fS2hcFvlJqCHCA9p4CV+5MZ1T2Ahi6OpfYLIUR9Ud0N3NXP\n1Qt6P1PmrsU7T1JgMvNIr6CqjUkIIWoYmbPiKkxFZr7emkBkKx/a+XtUdzhCCFGtJFlcxXd7TpGU\ncZHHbguu7lCEEKLaSbIog6nIzIcbYunYrCGDOsiaFUIIIcmiDMv3JpOQlstzg9pKd1khhECSRSmm\nIjP/WX9MShVCCGFFksUVLpUqnh0YIqUKIYSwkGRhRWvNRxtiCW3akMGhTao7HCGEqDEkWVg5kZbL\n8dQcxsvUHkIIUYIkCyt7EjMAiAjyruZIhBCiZpFkYWV3YgYezg608WtQ3aEIIUSNIsnCyp7ETMJb\neGFnJ1VQQghhTZKFRW6BiZgz5+kW6FXdoQghRI0jycJif1IWRWZN10BprxBCiCtJsrDYbWnc7tJC\nShZCCHElSRYWexIzCW7kjre7U3WHIoQQNY4kC4zBeHsSM+gi7RVCCFEmSRZAUsZFUi8U0E3aK4QQ\nokySLLjcXtFVShZCCFEmSRYY7RVuTva0ayIr4gkhRFkkWWBM89E5wBMHe/l1CCFEWer90zGvsIhD\nydkyvkIIIa6h3ieL7LxChnVuym1tGlV3KEIIUWM5VHcA1a2xhwvvj+la3WEIIUSNVu9LFkIIIcon\nyUIIIUS5bJoslFJDlVJHlFKxSqkZVznmQaVUtFLqkFJqgdX2CUqpY5afCbaMUwghxLXZrM1CKWUP\nfAQMBpKAHUqpFVrraKtjQoA/A7211hlKqcaW7T7AK0AEoIFdlnMzbBWvEEKIq7NlySISiNVax2ut\nC4CFwMgrjnkM+OhSEtBan7NsvwNYq7VOt+xbCwy1YaxCCCGuwZbJojlw0up9kmWbtbZAW6XUr0qp\nrUqpoddxrhBCiCpS3V1nHYAQoB8QAGxSSoVV9GSl1FRgKkBgYKAt4hNCCIFtSxangBZW7wMs26wl\nASu01oVa6+PAUYzkUZFz0VrP1lpHaK0j/Pz8KjV4IYQQlymttW0urJQDxsN/IMaDfgfwkNb6kNUx\nQ4GxWusJSqlGwB6gC5ZGbaCb5dDdQHetdfo1Pi8FSLjOMBsBqdd5Tm1XH+8Z6ud918d7hvp53zdz\nzy211uV+27ZZNZTW2qSUmg6sAeyBOVrrQ0qp14GdWusVln1DlFLRQBHwR611GoBS6g2MBAPw+rUS\nheXzrrtooZTaqbWOuN7zarP6eM9QP++7Pt4z1M/7rop7tlnJojaQ/6jqj/p43/XxnqF+3ndV3LOM\n4BZCCFGu+p4sZld3ANWgPt4z1M/7ro/3DPXzvm1+z/W6GkoIIUTF1PeShRBCiAqol8miIhMc1gVK\nqRZKqQ1WEzU+a9nuo5Raa5mkca1Sqs4tE6iUsldK7VFK/WB530optc3yN1+klHKq7hgrk1LKSym1\nVCkVo5Q6rJTqVU/+zs9b/ts+qJSKUkq51MW/tVJqjlLqnFLqoNW2Mv++yvCB5f73K6W6Xf3KFVfv\nkoXVBId3AqHAWKVUaPVGZTMm4A9a61DgFuApy73OAH7WWocAP1ve1zXPAoet3r8NvKe1bgNkAFOq\nJSrbeR/4SWvdHgjHuPc6/XdWSjUHngEitNadMLroj6Fu/q2/pPT8eFf7+96JMbg5BGOGi48rI4B6\nlyyo2ASHdYLW+rTWerfl9f+3d3chVlVhGMf/D6kwKmgpiCUyRdJFVApdSEWEdVWSQZGEUUjdeBF1\n0Qd1F9RNRIgVQSlhIEGYlVdRZFRQ2QdZUt2ZmOHnhfZJmT1drDW1GZy2k3Pm6D7PDw5z9tp7hrV5\nh/Oetfbe7/qJ8gFyHuV8N9bDNgI39aeHvSFpAXADsL5uC1gGbK6HdOqcJc0CrgY2ANj+w/YROh7n\nagowVB8Cng7so4OxtkYhNgMAAAOQSURBVP0+MPpZs7HiuwJ4ycXHwGxJ80+1D4OYLAaySKGkYWAJ\nsB2YZ3tf3bUfmNenbvXKWuBB4K+6PQc4YvvPut21mJ8PHAJerFNv6yXNoONxtv0D8CSwh5IkjlIq\nP3Q51k1jxbcnn3GDmCwGjqSZwKvAfbZ/bO5zuR2uM7fESVoOHLT9eb/7MommUErjPGd7CfALo6ac\nuhZngDpHv4KSLM8FZjCgSxlMRnwHMVmcVJHCrpA0lZIoNtneUpsPjAxL68+DY/3+GehK4EZJuylT\njMso8/mz61QFdC/me4G9trfX7c2U5NHlOANcB3xn+5DtY8AWSvy7HOumseLbk8+4QUwWnwKL6h0T\n0ygXxLb2uU89UefqNwDf2n6qsWsrMLJU7Z3AG5Pdt16x/bDtBbaHKbHdZnsV8C5wSz2sa+e8H/he\n0kW16VrgGzoc52oPsFTS9Pq/PnLenY31KGPFdytwR70railwtDFd9b8N5EN5kq6nzGuPFDh8vM9d\n6glJVwEfADv5d/7+Ecp1i1eAhZRKvbe2FWo8E0m6Brjf9nJJF1BGGudQqhvfbvv3fvZvIklaTLmg\nPw3YBaymfBnsdJwlPQqspNz59wVwN2V+vlOxlvQyZd2fucAByrLTr3OC+NbE+QxlSu5XYLXtz065\nD4OYLCIiYnwGcRoqIiLGKckiIiJaJVlERESrJIuIiGiVZBEREa2SLCLGQdJxSTsarwkrzidpuFlV\nNOJ0MqX9kIho+M324n53ImKyZWQRMQEk7Zb0hKSdkj6RdGFtH5a0ra4r8I6khbV9nqTXJH1ZX1fU\nP3WWpBfqGg1vSRrq20lFNCRZRIzP0KhpqJWNfUdtX0J5enZtbXsa2Gj7UmATsK62rwPes30ZpY7T\n17V9EfCs7YuBI8DNPT6fiJOSJ7gjxkHSz7ZnnqB9N7DM9q5avHG/7TmSDgPzbR+r7ftsz5V0CFjQ\nLENRy8i/XRezQdJDwFTbj/X+zCL+W0YWERPHY7wfj2YNo+PkumKcJpIsIibOysbPj+r7DynVbwFW\nUQo7QlkGcw38s174rMnqZMT/kW8tEeMzJGlHY/tN2yO3z54t6SvK6OC22nYPZQW7Byir2a2u7fcC\nz0u6izKCWENZ7S3itJRrFhEToF6zuNz24X73JaIXMg0VERGtMrKIiIhWGVlERESrJIuIiGiVZBER\nEa2SLCIiolWSRUREtEqyiIiIVn8D5x6g2Nl4Zi8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "40000/40000 [==============================] - 19s 482us/step - loss: 1.9580 - acc: 0.2628\n",
            "Epoch 1/1\n",
            "40000/40000 [==============================] - 18s 458us/step - loss: 1.6702 - acc: 0.3795\n",
            "Epoch 1/1\n",
            "40000/40000 [==============================] - 19s 477us/step - loss: 1.5527 - acc: 0.4249\n",
            "Epoch 1/1\n",
            "40000/40000 [==============================] - 19s 463us/step - loss: 1.4736 - acc: 0.4575\n",
            "Epoch 1/1\n",
            "40000/40000 [==============================] - 18s 451us/step - loss: 1.4124 - acc: 0.4847\n",
            "Epoch 1/1\n",
            "40000/40000 [==============================] - 18s 444us/step - loss: 1.3607 - acc: 0.5057\n",
            "Epoch 1/1\n",
            "40000/40000 [==============================] - 20s 494us/step - loss: 1.3170 - acc: 0.5212\n",
            "Epoch 1/1\n",
            "12000/40000 [========>.....................] - ETA: 14s - loss: 1.2821 - acc: 0.5357Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}