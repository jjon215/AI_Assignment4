{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fourth_Arch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjon215/MLHW_2/blob/master/Fourth_Arch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "G996wpuLDXK2",
        "colab_type": "code",
        "outputId": "0b19eb5d-6f37-40b7-efd6-250bd9eea2d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4463
        }
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import np_utils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "batch_size = 32\n",
        "num_classes = 10\n",
        "epochs = 100\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "x_train = list(x_train)\n",
        "y_train = list(y_train)\n",
        "\n",
        "val_loss = list()\n",
        "val_acc = list()\n",
        "\n",
        "x_val = np.array(x_train[4*10000:(4+1)*10000])\n",
        "y_val = np.array(y_train[4*10000:(4+1)*10000])\n",
        "  \n",
        "x_tra = np.array(x_train[0:4*10000]+x_train[(4+1)*10000:50000])\n",
        "y_tra = np.array(y_train[0:4*10000]+y_train[(4+1)*10000:50000])\n",
        "  \n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                 input_shape=x_tra.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(128, (2, 2), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(128, (2, 2)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "   \n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=Adam(lr=0.0001, decay=1e-6),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  \n",
        "        samplewise_center=False,  \n",
        "        featurewise_std_normalization=False, \n",
        "        samplewise_std_normalization=False,  \n",
        "        zca_whitening=False,  \n",
        "        zca_epsilon=1e-06, \n",
        "        rotation_range=0,  \n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,\n",
        "        shear_range=0.,  \n",
        "        zoom_range=0., \n",
        "        channel_shift_range=0., \n",
        "        fill_mode='nearest',\n",
        "        cval=0.,  \n",
        "        horizontal_flip=True,  \n",
        "        vertical_flip=False, \n",
        "        rescale=None,\n",
        "        preprocessing_function=None,\n",
        "        data_format=None,\n",
        "        validation_split=0.0)\n",
        "\n",
        "datagen.fit(x_tra)\n",
        "\n",
        "for e in range(10):\n",
        "    batches = 0\n",
        "    for x_batch, y_batch in datagen.flow(x_tra, y_tra, batch_size=40000):\n",
        "        model.fit(x_batch, y_batch)\n",
        "        batches += 1\n",
        "        if batches >= 1:\n",
        "            break\n",
        "\n",
        "History = model.fit(x_tra, y_tra,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,                    \n",
        "          verbose=1,\n",
        "          validation_data=(x_val, y_val))\n",
        "score = model.evaluate(x_val, y_val, verbose=0)\n",
        "print('Test loss is :', score[0])\n",
        "print('Test accuracy is :', score[1])\n",
        "print(score)\n",
        "print(History.history)\n",
        "\n",
        "plotaccuracy = plt.plot(range(1,epochs+1),History.history['acc'],range(1,epochs+1),History.history['val_acc'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(('Train Accuracy','Test Accuracy'))\n",
        "plt.show(plotaccuracy)\n",
        "\n",
        "print('Validation loss:',History.history['val_loss'][-1])\n",
        "print('Validation accuracy:',History.history['val_acc'][-1])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/1\n",
            "40000/40000 [==============================] - 24s 588us/step - loss: 1.9951 - acc: 0.2412\n",
            "Epoch 1/1\n",
            "40000/40000 [==============================] - 23s 587us/step - loss: 1.6861 - acc: 0.3709\n",
            "Epoch 1/1\n",
            "40000/40000 [==============================] - 28s 690us/step - loss: 1.5737 - acc: 0.4171\n",
            "Epoch 1/1\n",
            "40000/40000 [==============================] - 21s 523us/step - loss: 1.4981 - acc: 0.4458\n",
            "Epoch 1/1\n",
            "40000/40000 [==============================] - 25s 633us/step - loss: 1.4318 - acc: 0.4764\n",
            "Epoch 1/1\n",
            "40000/40000 [==============================] - 28s 692us/step - loss: 1.3745 - acc: 0.4999\n",
            "Epoch 1/1\n",
            "40000/40000 [==============================] - 28s 697us/step - loss: 1.3312 - acc: 0.5177\n",
            "Epoch 1/1\n",
            "40000/40000 [==============================] - 22s 546us/step - loss: 1.2880 - acc: 0.5375\n",
            "Epoch 1/1\n",
            "40000/40000 [==============================] - 22s 556us/step - loss: 1.2530 - acc: 0.5483\n",
            "Epoch 1/1\n",
            "40000/40000 [==============================] - 27s 684us/step - loss: 1.2162 - acc: 0.5646\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "40000/40000 [==============================] - 30s 756us/step - loss: 1.1021 - acc: 0.6065 - val_loss: 0.9940 - val_acc: 0.6486\n",
            "Epoch 2/100\n",
            "40000/40000 [==============================] - 29s 715us/step - loss: 1.0673 - acc: 0.6198 - val_loss: 0.9900 - val_acc: 0.6478\n",
            "Epoch 3/100\n",
            "40000/40000 [==============================] - 25s 623us/step - loss: 1.0369 - acc: 0.6303 - val_loss: 0.9496 - val_acc: 0.6607\n",
            "Epoch 4/100\n",
            "40000/40000 [==============================] - 29s 714us/step - loss: 1.0073 - acc: 0.6391 - val_loss: 0.9472 - val_acc: 0.6642\n",
            "Epoch 5/100\n",
            "40000/40000 [==============================] - 29s 725us/step - loss: 0.9830 - acc: 0.6485 - val_loss: 0.8893 - val_acc: 0.6865\n",
            "Epoch 6/100\n",
            "40000/40000 [==============================] - 30s 741us/step - loss: 0.9580 - acc: 0.6597 - val_loss: 0.8686 - val_acc: 0.6940\n",
            "Epoch 7/100\n",
            "40000/40000 [==============================] - 30s 755us/step - loss: 0.9366 - acc: 0.6674 - val_loss: 0.8710 - val_acc: 0.6935\n",
            "Epoch 8/100\n",
            "40000/40000 [==============================] - 30s 753us/step - loss: 0.9133 - acc: 0.6736 - val_loss: 0.8327 - val_acc: 0.7048\n",
            "Epoch 9/100\n",
            "40000/40000 [==============================] - 30s 745us/step - loss: 0.8968 - acc: 0.6821 - val_loss: 0.8306 - val_acc: 0.7067\n",
            "Epoch 10/100\n",
            "40000/40000 [==============================] - 30s 753us/step - loss: 0.8745 - acc: 0.6914 - val_loss: 0.8131 - val_acc: 0.7127\n",
            "Epoch 11/100\n",
            "40000/40000 [==============================] - 30s 750us/step - loss: 0.8592 - acc: 0.6969 - val_loss: 0.8265 - val_acc: 0.7087\n",
            "Epoch 12/100\n",
            "40000/40000 [==============================] - 30s 748us/step - loss: 0.8421 - acc: 0.6988 - val_loss: 0.7923 - val_acc: 0.7206\n",
            "Epoch 13/100\n",
            "40000/40000 [==============================] - 28s 701us/step - loss: 0.8263 - acc: 0.7077 - val_loss: 0.7714 - val_acc: 0.7302\n",
            "Epoch 14/100\n",
            "40000/40000 [==============================] - 29s 725us/step - loss: 0.8054 - acc: 0.7147 - val_loss: 0.7620 - val_acc: 0.7368\n",
            "Epoch 15/100\n",
            "40000/40000 [==============================] - 30s 759us/step - loss: 0.7940 - acc: 0.7200 - val_loss: 0.7443 - val_acc: 0.7433\n",
            "Epoch 16/100\n",
            "40000/40000 [==============================] - 30s 756us/step - loss: 0.7769 - acc: 0.7248 - val_loss: 0.7283 - val_acc: 0.7484\n",
            "Epoch 17/100\n",
            "40000/40000 [==============================] - 30s 758us/step - loss: 0.7697 - acc: 0.7268 - val_loss: 0.7179 - val_acc: 0.7497\n",
            "Epoch 18/100\n",
            "40000/40000 [==============================] - 30s 755us/step - loss: 0.7483 - acc: 0.7350 - val_loss: 0.7134 - val_acc: 0.7504\n",
            "Epoch 19/100\n",
            "40000/40000 [==============================] - 30s 748us/step - loss: 0.7352 - acc: 0.7419 - val_loss: 0.6999 - val_acc: 0.7514\n",
            "Epoch 20/100\n",
            "40000/40000 [==============================] - 30s 755us/step - loss: 0.7249 - acc: 0.7456 - val_loss: 0.6819 - val_acc: 0.7637\n",
            "Epoch 21/100\n",
            "40000/40000 [==============================] - 30s 754us/step - loss: 0.7093 - acc: 0.7488 - val_loss: 0.6760 - val_acc: 0.7648\n",
            "Epoch 22/100\n",
            "40000/40000 [==============================] - 30s 742us/step - loss: 0.7032 - acc: 0.7504 - val_loss: 0.6666 - val_acc: 0.7688\n",
            "Epoch 23/100\n",
            "40000/40000 [==============================] - 30s 742us/step - loss: 0.6881 - acc: 0.7578 - val_loss: 0.6609 - val_acc: 0.7756\n",
            "Epoch 24/100\n",
            "40000/40000 [==============================] - 30s 758us/step - loss: 0.6827 - acc: 0.7573 - val_loss: 0.6587 - val_acc: 0.7746\n",
            "Epoch 25/100\n",
            "40000/40000 [==============================] - 30s 756us/step - loss: 0.6657 - acc: 0.7650 - val_loss: 0.6668 - val_acc: 0.7670\n",
            "Epoch 26/100\n",
            "40000/40000 [==============================] - 30s 759us/step - loss: 0.6569 - acc: 0.7675 - val_loss: 0.6317 - val_acc: 0.7801\n",
            "Epoch 27/100\n",
            "40000/40000 [==============================] - 30s 760us/step - loss: 0.6471 - acc: 0.7710 - val_loss: 0.6327 - val_acc: 0.7806\n",
            "Epoch 28/100\n",
            "40000/40000 [==============================] - 30s 760us/step - loss: 0.6422 - acc: 0.7738 - val_loss: 0.6317 - val_acc: 0.7787\n",
            "Epoch 29/100\n",
            "40000/40000 [==============================] - 30s 752us/step - loss: 0.6326 - acc: 0.7758 - val_loss: 0.6235 - val_acc: 0.7841\n",
            "Epoch 30/100\n",
            "40000/40000 [==============================] - 30s 750us/step - loss: 0.6258 - acc: 0.7793 - val_loss: 0.6367 - val_acc: 0.7818\n",
            "Epoch 31/100\n",
            "40000/40000 [==============================] - 29s 719us/step - loss: 0.6112 - acc: 0.7821 - val_loss: 0.6301 - val_acc: 0.7801\n",
            "Epoch 32/100\n",
            "40000/40000 [==============================] - 21s 536us/step - loss: 0.6007 - acc: 0.7873 - val_loss: 0.5985 - val_acc: 0.7930\n",
            "Epoch 33/100\n",
            "40000/40000 [==============================] - 26s 649us/step - loss: 0.5969 - acc: 0.7894 - val_loss: 0.5976 - val_acc: 0.7933\n",
            "Epoch 34/100\n",
            "40000/40000 [==============================] - 22s 560us/step - loss: 0.5895 - acc: 0.7915 - val_loss: 0.6299 - val_acc: 0.7812\n",
            "Epoch 35/100\n",
            "40000/40000 [==============================] - 23s 577us/step - loss: 0.5790 - acc: 0.7952 - val_loss: 0.5978 - val_acc: 0.7955\n",
            "Epoch 36/100\n",
            "40000/40000 [==============================] - 25s 615us/step - loss: 0.5750 - acc: 0.7962 - val_loss: 0.5877 - val_acc: 0.7940\n",
            "Epoch 37/100\n",
            "40000/40000 [==============================] - 27s 663us/step - loss: 0.5698 - acc: 0.7981 - val_loss: 0.5888 - val_acc: 0.7977\n",
            "Epoch 38/100\n",
            "40000/40000 [==============================] - 29s 722us/step - loss: 0.5600 - acc: 0.8023 - val_loss: 0.6046 - val_acc: 0.7913\n",
            "Epoch 39/100\n",
            "40000/40000 [==============================] - 30s 757us/step - loss: 0.5498 - acc: 0.8050 - val_loss: 0.5759 - val_acc: 0.8013\n",
            "Epoch 40/100\n",
            "40000/40000 [==============================] - 30s 755us/step - loss: 0.5458 - acc: 0.8070 - val_loss: 0.5808 - val_acc: 0.7995\n",
            "Epoch 41/100\n",
            "40000/40000 [==============================] - 29s 727us/step - loss: 0.5419 - acc: 0.8080 - val_loss: 0.5979 - val_acc: 0.7929\n",
            "Epoch 42/100\n",
            "40000/40000 [==============================] - 29s 718us/step - loss: 0.5349 - acc: 0.8106 - val_loss: 0.6035 - val_acc: 0.7950\n",
            "Epoch 43/100\n",
            "40000/40000 [==============================] - 29s 730us/step - loss: 0.5261 - acc: 0.8140 - val_loss: 0.5750 - val_acc: 0.8000\n",
            "Epoch 44/100\n",
            "40000/40000 [==============================] - 29s 718us/step - loss: 0.5222 - acc: 0.8147 - val_loss: 0.5589 - val_acc: 0.8112\n",
            "Epoch 45/100\n",
            "40000/40000 [==============================] - 29s 716us/step - loss: 0.5150 - acc: 0.8186 - val_loss: 0.5575 - val_acc: 0.8080\n",
            "Epoch 46/100\n",
            "40000/40000 [==============================] - 27s 680us/step - loss: 0.5134 - acc: 0.8174 - val_loss: 0.5709 - val_acc: 0.8045\n",
            "Epoch 47/100\n",
            "40000/40000 [==============================] - 25s 631us/step - loss: 0.4995 - acc: 0.8227 - val_loss: 0.5768 - val_acc: 0.8015\n",
            "Epoch 48/100\n",
            "40000/40000 [==============================] - 25s 637us/step - loss: 0.4949 - acc: 0.8232 - val_loss: 0.5945 - val_acc: 0.7981\n",
            "Epoch 49/100\n",
            "40000/40000 [==============================] - 21s 520us/step - loss: 0.4913 - acc: 0.8247 - val_loss: 0.5686 - val_acc: 0.8071\n",
            "Epoch 50/100\n",
            "40000/40000 [==============================] - 23s 568us/step - loss: 0.4850 - acc: 0.8258 - val_loss: 0.5632 - val_acc: 0.8106\n",
            "Epoch 51/100\n",
            "40000/40000 [==============================] - 26s 647us/step - loss: 0.4814 - acc: 0.8297 - val_loss: 0.5581 - val_acc: 0.8118\n",
            "Epoch 52/100\n",
            "40000/40000 [==============================] - 24s 612us/step - loss: 0.4792 - acc: 0.8303 - val_loss: 0.5596 - val_acc: 0.8085\n",
            "Epoch 53/100\n",
            "40000/40000 [==============================] - 28s 707us/step - loss: 0.4726 - acc: 0.8319 - val_loss: 0.5584 - val_acc: 0.8108\n",
            "Epoch 54/100\n",
            "40000/40000 [==============================] - 28s 698us/step - loss: 0.4671 - acc: 0.8340 - val_loss: 0.5529 - val_acc: 0.8105\n",
            "Epoch 55/100\n",
            "40000/40000 [==============================] - 30s 746us/step - loss: 0.4602 - acc: 0.8356 - val_loss: 0.5423 - val_acc: 0.8142\n",
            "Epoch 56/100\n",
            "40000/40000 [==============================] - 30s 752us/step - loss: 0.4616 - acc: 0.8357 - val_loss: 0.5611 - val_acc: 0.8077\n",
            "Epoch 57/100\n",
            "40000/40000 [==============================] - 30s 751us/step - loss: 0.4534 - acc: 0.8392 - val_loss: 0.5514 - val_acc: 0.8122\n",
            "Epoch 58/100\n",
            "40000/40000 [==============================] - 30s 745us/step - loss: 0.4467 - acc: 0.8403 - val_loss: 0.5548 - val_acc: 0.8140\n",
            "Epoch 59/100\n",
            "40000/40000 [==============================] - 30s 749us/step - loss: 0.4417 - acc: 0.8439 - val_loss: 0.5420 - val_acc: 0.8154\n",
            "Epoch 60/100\n",
            "40000/40000 [==============================] - 29s 733us/step - loss: 0.4378 - acc: 0.8444 - val_loss: 0.5420 - val_acc: 0.8155\n",
            "Epoch 61/100\n",
            "40000/40000 [==============================] - 30s 755us/step - loss: 0.4325 - acc: 0.8435 - val_loss: 0.5376 - val_acc: 0.8160\n",
            "Epoch 62/100\n",
            "40000/40000 [==============================] - 30s 753us/step - loss: 0.4282 - acc: 0.8468 - val_loss: 0.5485 - val_acc: 0.8137\n",
            "Epoch 63/100\n",
            "40000/40000 [==============================] - 30s 757us/step - loss: 0.4277 - acc: 0.8486 - val_loss: 0.5332 - val_acc: 0.8202\n",
            "Epoch 64/100\n",
            "40000/40000 [==============================] - 30s 745us/step - loss: 0.4220 - acc: 0.8484 - val_loss: 0.5267 - val_acc: 0.8223\n",
            "Epoch 65/100\n",
            "40000/40000 [==============================] - 29s 730us/step - loss: 0.4166 - acc: 0.8500 - val_loss: 0.5396 - val_acc: 0.8155\n",
            "Epoch 66/100\n",
            "40000/40000 [==============================] - 26s 641us/step - loss: 0.4092 - acc: 0.8533 - val_loss: 0.5540 - val_acc: 0.8154\n",
            "Epoch 67/100\n",
            "40000/40000 [==============================] - 20s 509us/step - loss: 0.4087 - acc: 0.8529 - val_loss: 0.5221 - val_acc: 0.8275\n",
            "Epoch 68/100\n",
            "40000/40000 [==============================] - 21s 532us/step - loss: 0.4021 - acc: 0.8556 - val_loss: 0.5323 - val_acc: 0.8228\n",
            "Epoch 69/100\n",
            "40000/40000 [==============================] - 23s 575us/step - loss: 0.4030 - acc: 0.8555 - val_loss: 0.5476 - val_acc: 0.8177\n",
            "Epoch 70/100\n",
            "40000/40000 [==============================] - 24s 592us/step - loss: 0.3947 - acc: 0.8600 - val_loss: 0.5370 - val_acc: 0.8236\n",
            "Epoch 71/100\n",
            "40000/40000 [==============================] - 29s 727us/step - loss: 0.3905 - acc: 0.8616 - val_loss: 0.5409 - val_acc: 0.8223\n",
            "Epoch 72/100\n",
            "40000/40000 [==============================] - 30s 758us/step - loss: 0.3909 - acc: 0.8607 - val_loss: 0.5300 - val_acc: 0.8207\n",
            "Epoch 73/100\n",
            "40000/40000 [==============================] - 30s 761us/step - loss: 0.3862 - acc: 0.8622 - val_loss: 0.5331 - val_acc: 0.8254\n",
            "Epoch 74/100\n",
            "40000/40000 [==============================] - 30s 750us/step - loss: 0.3797 - acc: 0.8648 - val_loss: 0.5335 - val_acc: 0.8211\n",
            "Epoch 75/100\n",
            "40000/40000 [==============================] - 30s 751us/step - loss: 0.3776 - acc: 0.8669 - val_loss: 0.5302 - val_acc: 0.8233\n",
            "Epoch 76/100\n",
            "40000/40000 [==============================] - 29s 733us/step - loss: 0.3673 - acc: 0.8699 - val_loss: 0.5285 - val_acc: 0.8242\n",
            "Epoch 77/100\n",
            "40000/40000 [==============================] - 28s 699us/step - loss: 0.3681 - acc: 0.8683 - val_loss: 0.5289 - val_acc: 0.8254\n",
            "Epoch 78/100\n",
            "40000/40000 [==============================] - 23s 567us/step - loss: 0.3655 - acc: 0.8708 - val_loss: 0.5304 - val_acc: 0.8266\n",
            "Epoch 79/100\n",
            "40000/40000 [==============================] - 26s 648us/step - loss: 0.3611 - acc: 0.8707 - val_loss: 0.5396 - val_acc: 0.8233\n",
            "Epoch 80/100\n",
            "40000/40000 [==============================] - 27s 669us/step - loss: 0.3644 - acc: 0.8704 - val_loss: 0.5222 - val_acc: 0.8265\n",
            "Epoch 81/100\n",
            "40000/40000 [==============================] - 26s 640us/step - loss: 0.3591 - acc: 0.8702 - val_loss: 0.5303 - val_acc: 0.8273\n",
            "Epoch 82/100\n",
            "40000/40000 [==============================] - 23s 566us/step - loss: 0.3540 - acc: 0.8743 - val_loss: 0.5370 - val_acc: 0.8265\n",
            "Epoch 83/100\n",
            "40000/40000 [==============================] - 23s 566us/step - loss: 0.3512 - acc: 0.8744 - val_loss: 0.5405 - val_acc: 0.8268\n",
            "Epoch 84/100\n",
            "40000/40000 [==============================] - 26s 638us/step - loss: 0.3478 - acc: 0.8748 - val_loss: 0.5326 - val_acc: 0.8288\n",
            "Epoch 85/100\n",
            "40000/40000 [==============================] - 26s 651us/step - loss: 0.3455 - acc: 0.8762 - val_loss: 0.5300 - val_acc: 0.8278\n",
            "Epoch 86/100\n",
            "40000/40000 [==============================] - 27s 675us/step - loss: 0.3422 - acc: 0.8771 - val_loss: 0.5244 - val_acc: 0.8273\n",
            "Epoch 87/100\n",
            "40000/40000 [==============================] - 26s 657us/step - loss: 0.3406 - acc: 0.8755 - val_loss: 0.5351 - val_acc: 0.8276\n",
            "Epoch 88/100\n",
            "40000/40000 [==============================] - 27s 673us/step - loss: 0.3376 - acc: 0.8779 - val_loss: 0.5230 - val_acc: 0.8315\n",
            "Epoch 89/100\n",
            "40000/40000 [==============================] - 30s 742us/step - loss: 0.3382 - acc: 0.8805 - val_loss: 0.5356 - val_acc: 0.8259\n",
            "Epoch 90/100\n",
            "40000/40000 [==============================] - 30s 744us/step - loss: 0.3342 - acc: 0.8803 - val_loss: 0.5321 - val_acc: 0.8261\n",
            "Epoch 91/100\n",
            "40000/40000 [==============================] - 30s 738us/step - loss: 0.3286 - acc: 0.8811 - val_loss: 0.5515 - val_acc: 0.8277\n",
            "Epoch 92/100\n",
            "40000/40000 [==============================] - 30s 748us/step - loss: 0.3244 - acc: 0.8842 - val_loss: 0.5336 - val_acc: 0.8277\n",
            "Epoch 93/100\n",
            "40000/40000 [==============================] - 30s 754us/step - loss: 0.3254 - acc: 0.8829 - val_loss: 0.5315 - val_acc: 0.8271\n",
            "Epoch 94/100\n",
            "40000/40000 [==============================] - 30s 753us/step - loss: 0.3189 - acc: 0.8851 - val_loss: 0.5455 - val_acc: 0.8238\n",
            "Epoch 95/100\n",
            "40000/40000 [==============================] - 30s 753us/step - loss: 0.3159 - acc: 0.8866 - val_loss: 0.5369 - val_acc: 0.8302\n",
            "Epoch 96/100\n",
            "40000/40000 [==============================] - 30s 756us/step - loss: 0.3205 - acc: 0.8841 - val_loss: 0.5278 - val_acc: 0.8293\n",
            "Epoch 97/100\n",
            "40000/40000 [==============================] - 29s 734us/step - loss: 0.3139 - acc: 0.8864 - val_loss: 0.5302 - val_acc: 0.8306\n",
            "Epoch 98/100\n",
            "40000/40000 [==============================] - 30s 747us/step - loss: 0.3125 - acc: 0.8877 - val_loss: 0.5325 - val_acc: 0.8299\n",
            "Epoch 99/100\n",
            "40000/40000 [==============================] - 30s 745us/step - loss: 0.3094 - acc: 0.8893 - val_loss: 0.5361 - val_acc: 0.8286\n",
            "Epoch 100/100\n",
            "40000/40000 [==============================] - 26s 645us/step - loss: 0.3069 - acc: 0.8886 - val_loss: 0.5385 - val_acc: 0.8275\n",
            "Test loss is : 0.5385086541652679\n",
            "Test accuracy is : 0.8275\n",
            "[0.5385086541652679, 0.8275]\n",
            "{'val_loss': [0.9939698965072632, 0.9899538682937622, 0.9495964927673339, 0.9472305484771728, 0.8892685526847839, 0.868572387599945, 0.8709766496658325, 0.8327097675323486, 0.8306362267494202, 0.8130930104255676, 0.8264658857345581, 0.7922643465995789, 0.7713587056159973, 0.761962645149231, 0.7442897641181946, 0.7282590859413147, 0.7178939298629761, 0.7133669423103333, 0.6998578783035279, 0.6818641085624695, 0.6759704283714294, 0.6665654196739197, 0.6608535960197449, 0.6587203806877137, 0.666786321926117, 0.6316502539634704, 0.6327182358264923, 0.631749922466278, 0.6235378041744232, 0.6366745915412902, 0.630080762386322, 0.5985001932144165, 0.5976498933792114, 0.6299228235721588, 0.5977741414070129, 0.5877475843906402, 0.5888023024559021, 0.6045725622177124, 0.5758695861816406, 0.5807813697338104, 0.5979358139038086, 0.6034615567684174, 0.5749980624675751, 0.5589065753459931, 0.5574536758899689, 0.5709194051265717, 0.5767732778549194, 0.5945341693878173, 0.5685932146549225, 0.5631947715759278, 0.5580864391803741, 0.5595555626869202, 0.5583566505432129, 0.5529137345790863, 0.5423051448345184, 0.5610772903442383, 0.5514282723426819, 0.5548346144676208, 0.5419580794811248, 0.5419893436431885, 0.537596787405014, 0.5485307260513306, 0.5331820711135864, 0.5266620470046997, 0.5395928480625153, 0.5540303107261658, 0.5220572996616364, 0.5323327222824097, 0.547575446176529, 0.5370068212509155, 0.5409469416618348, 0.5299835751056671, 0.5330560203552246, 0.5334798599720001, 0.5302476845741272, 0.5284679456233978, 0.5289451941490173, 0.530448308467865, 0.5396376401901245, 0.5222492800235748, 0.5302776766777039, 0.5370438637256623, 0.5404846720695495, 0.5325895452022552, 0.5300078331947327, 0.5243583576202393, 0.5351304696559906, 0.5229594608306884, 0.5356160252094269, 0.5321338805675506, 0.5514531629085541, 0.5335688197612762, 0.5315162607192994, 0.545460534620285, 0.5369236942768096, 0.5278252145290375, 0.5302432583332062, 0.5325456689834595, 0.536085149526596, 0.5385086541652679], 'val_acc': [0.6486, 0.6478, 0.6607, 0.6642, 0.6865, 0.694, 0.6935, 0.7048, 0.7067, 0.7127, 0.7087, 0.7206, 0.7302, 0.7368, 0.7433, 0.7484, 0.7497, 0.7504, 0.7514, 0.7637, 0.7648, 0.7688, 0.7756, 0.7746, 0.767, 0.7801, 0.7806, 0.7787, 0.7841, 0.7818, 0.7801, 0.793, 0.7933, 0.7812, 0.7955, 0.794, 0.7977, 0.7913, 0.8013, 0.7995, 0.7929, 0.795, 0.8, 0.8112, 0.808, 0.8045, 0.8015, 0.7981, 0.8071, 0.8106, 0.8118, 0.8085, 0.8108, 0.8105, 0.8142, 0.8077, 0.8122, 0.814, 0.8154, 0.8155, 0.816, 0.8137, 0.8202, 0.8223, 0.8155, 0.8154, 0.8275, 0.8228, 0.8177, 0.8236, 0.8223, 0.8207, 0.8254, 0.8211, 0.8233, 0.8242, 0.8254, 0.8266, 0.8233, 0.8265, 0.8273, 0.8265, 0.8268, 0.8288, 0.8278, 0.8273, 0.8276, 0.8315, 0.8259, 0.8261, 0.8277, 0.8277, 0.8271, 0.8238, 0.8302, 0.8293, 0.8306, 0.8299, 0.8286, 0.8275], 'loss': [1.1020671936035156, 1.0673360930919646, 1.0369046701431275, 1.0073105293750764, 0.9829883357048035, 0.9580129563808442, 0.9365665103435517, 0.9133069972276687, 0.8968497847080231, 0.8744571492910385, 0.8592466296672822, 0.8420573821306229, 0.8263190006732941, 0.805419418668747, 0.7940322785377503, 0.7768975804567337, 0.7697139610290528, 0.7483460981369019, 0.7352054970502854, 0.7248975355863572, 0.7093032891750336, 0.7031537575006485, 0.6880538463592529, 0.6827355967998505, 0.6656695600509643, 0.6568600628256798, 0.6471312352895736, 0.6421539981007576, 0.6325692776679993, 0.6257590599536895, 0.6111791771054268, 0.6007313249111176, 0.5969360751867294, 0.5894911200404167, 0.5790197805523872, 0.574957860481739, 0.5697540820956231, 0.560003313934803, 0.5497651711463928, 0.5458050886273385, 0.5419488819241524, 0.5349370509028435, 0.5260513767600059, 0.5222031824231148, 0.5149864109277725, 0.5134328282475471, 0.49945776499509814, 0.49492644624114035, 0.49128162647485735, 0.4849791902899742, 0.481359289252758, 0.47920889279842377, 0.4726454053997993, 0.46706919672489167, 0.4602498933553696, 0.4615996738731861, 0.4533985257267952, 0.4466687538266182, 0.44172061494588855, 0.43778920969963075, 0.43253293529748915, 0.42816857438087463, 0.42773552404642107, 0.4220461955845356, 0.41657441086769104, 0.40918242906928065, 0.40872629500627516, 0.4021426693737507, 0.403046147954464, 0.39472157186269763, 0.39046550996899604, 0.39085016380548476, 0.38621966910362243, 0.3796824505150318, 0.37758843499422073, 0.3672625679075718, 0.36808759930431845, 0.36554764216542246, 0.3610540494680405, 0.3644182190656662, 0.35908522455990316, 0.3540332958936691, 0.35124304172992704, 0.3477595264673233, 0.34545935947299006, 0.3421772956341505, 0.34062052735686305, 0.3375714928805828, 0.33819021493792534, 0.3341901553273201, 0.3286491377234459, 0.32442321707606314, 0.3253865128159523, 0.31889670222401617, 0.3159097882866859, 0.3205120017915964, 0.3138822016358376, 0.3124993359208107, 0.3094496868699789, 0.3068682314783335], 'acc': [0.606475, 0.619775, 0.6303, 0.63905, 0.64855, 0.6597, 0.667375, 0.673625, 0.6821, 0.691425, 0.6969, 0.6988, 0.707675, 0.714675, 0.72, 0.7248, 0.72675, 0.735, 0.74185, 0.74565, 0.748775, 0.7504, 0.75775, 0.757275, 0.765, 0.767525, 0.771, 0.773825, 0.775775, 0.779325, 0.782125, 0.7873, 0.78945, 0.791475, 0.79515, 0.796175, 0.798125, 0.8023, 0.805, 0.806975, 0.80795, 0.8106, 0.814025, 0.814675, 0.8186, 0.817425, 0.82265, 0.823225, 0.82475, 0.825775, 0.829675, 0.8303, 0.83195, 0.83405, 0.835625, 0.8357, 0.8392, 0.84035, 0.8439, 0.84435, 0.843525, 0.846825, 0.848625, 0.848375, 0.85005, 0.853325, 0.8529, 0.8556, 0.855475, 0.859975, 0.86155, 0.8607, 0.862175, 0.8648, 0.8669, 0.869925, 0.868325, 0.8708, 0.87065, 0.870425, 0.87025, 0.874275, 0.874375, 0.874775, 0.8762, 0.87715, 0.875525, 0.877875, 0.88045, 0.880275, 0.88115, 0.8842, 0.88295, 0.885075, 0.88665, 0.8841, 0.886425, 0.887675, 0.8893, 0.8886]}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.5385086541652679\n",
            "Validation accuracy: 0.8275\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}